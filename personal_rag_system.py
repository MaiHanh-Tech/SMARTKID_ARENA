"""
PERSONAL RAG SYSTEM - AI h·ªçc phong c√°ch t∆∞ duy c·ªßa User
Tri·∫øt l√Ω: "Being No One" (Metzinger) - Ego l√† construct c√≥ th·ªÉ m√¥ h√¨nh h√≥a
"""

import streamlit as st
from datetime import datetime
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class PersonalRAG:
    """
    H·ªá th·ªëng RAG c√° nh√¢n h√≥a:
    1. Thu th·∫≠p: M·ªçi t∆∞∆°ng t√°c (chat, debate, translation) ‚Üí Memory
    2. Tr√≠ch xu·∫•t: Patterns (keywords ∆∞a th√≠ch, phong c√°ch lu·∫≠n ƒëi·ªÉm)
    3. T·ªïng h·ª£p: User Profile vector
    4. √Åp d·ª•ng: Prompt injection ƒë·ªÉ AI m√¥ ph·ªèng user
    """
    
    def __init__(self, supabase_client, user_id):
        self.db = supabase_client
        self.user_id = user_id
        self.encoder = self._load_encoder()
        self.profile = self._load_user_profile()
    
    @st.cache_resource
    def _load_encoder(_self):
        return SentenceTransformer(
            "paraphrase-multilingual-MiniLM-L12-v2",
            device='cpu'
        )
    
    def _load_user_profile(self):
        """
        Load user profile t·ª´ DB ho·∫∑c t·∫°o m·ªõi
        
        Profile structure:
        {
            "user_id": "alice",
            "thinking_style": {
                "favorite_keywords": ["entropy", "systems", "causality"],
                "writing_tone": "analytical, scientific, philosophical",
                "debate_strategy": "First-principles reasoning"
            },
            "knowledge_interests": ["Physics", "Philosophy", "Complex Systems"],
            "interaction_history_embeddings": [...],  # Vector trung b√¨nh
            "last_updated": "2026-01-01T00:00:00"
        }
        """
        try:
            # L·∫•y t·ª´ Supabase table "user_profiles"
            response = self.db.table("user_profiles").select("*").eq("user_id", self.user_id).execute()
            
            if response.data:
                return json.loads(response.data[0]["profile_json"])
            else:
                # T·∫°o profile m·ªõi
                default_profile = {
                    "user_id": self.user_id,
                    "thinking_style": {},
                    "knowledge_interests": [],
                    "interaction_history_embeddings": [],
                    "last_updated": datetime.now().isoformat()
                }
                return default_profile
        except Exception as e:
            st.warning(f"Kh√¥ng load ƒë∆∞·ª£c profile: {e}")
            return {}
    
    def record_interaction(self, interaction_type, content, context=None):
        """
        Ghi l·∫°i m·ªçi t∆∞∆°ng t√°c c·ªßa user
        
        Args:
            interaction_type: "debate", "translation", "book_analysis", "query"
            content: N·ªôi dung user vi·∫øt/n√≥i
            context: Dict ch·ª©a th√¥ng tin b·ªï sung (VD: persona_used, result)
        """
        if not content or len(content.strip()) < 10:
            return  # B·ªè qua input qu√° ng·∫Øn
        
        # 1. T·∫°o embedding
        embedding = self.encoder.encode([content])[0].tolist()
        
        # 2. L∆∞u v√†o DB (table "user_interactions")
        data = {
            "user_id": self.user_id,
            "type": interaction_type,
            "content": content,
            "embedding": json.dumps(embedding),  # Supabase JSON field
            "context": json.dumps(context or {}),
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            self.db.table("user_interactions").insert(data).execute()
        except Exception as e:
            st.warning(f"Kh√¥ng ghi ƒë∆∞·ª£c interaction: {e}")
    
    def update_profile(self, force=False):
        """
        C·∫≠p nh·∫≠t user profile d·ª±a tr√™n l·ªãch s·ª≠ t∆∞∆°ng t√°c
        
        Logic:
        1. L·∫•y 100 t∆∞∆°ng t√°c g·∫ßn nh·∫•t
        2. Clustering ‚Üí T√¨m ch·ªß ƒë·ªÅ ∆∞a th√≠ch
        3. Keyword extraction ‚Üí T·ª´ ng·ªØ ƒë·∫∑c tr∆∞ng
        4. Tone analysis ‚Üí Phong c√°ch vi·∫øt
        """
        # Ch·ªâ update 1 l·∫ßn/ng√†y (tr·ª´ khi force=True)
        if not force:
            last_update = datetime.fromisoformat(self.profile.get("last_updated", "2020-01-01"))
            if (datetime.now() - last_update).days < 1:
                return
        
        try:
            # 1. L·∫•y l·ªãch s·ª≠
            response = self.db.table("user_interactions")\
                .select("*")\
                .eq("user_id", self.user_id)\
                .order("timestamp", desc=True)\
                .limit(100)\
                .execute()
            
            if not response.data or len(response.data) < 10:
                st.info("Ch∆∞a ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ x√¢y d·ª±ng profile (c·∫ßn √≠t nh·∫•t 10 t∆∞∆°ng t√°c)")
                return
            
            interactions = response.data
            
            # 2. Ph√¢n t√≠ch
            contents = [item["content"] for item in interactions]
            embeddings = [json.loads(item["embedding"]) for item in interactions]
            
            # 2a. Vector trung b√¨nh (ƒë·∫°i di·ªán phong c√°ch t∆∞ duy)
            avg_embedding = np.mean(embeddings, axis=0).tolist()
            
            # 2b. Keyword extraction (ƒë∆°n gi·∫£n: word frequency)
            from collections import Counter
            all_words = " ".join(contents).lower().split()
            common_words = [word for word, count in Counter(all_words).most_common(20)
                          if len(word) > 4]  # L·ªçc t·ª´ ng·∫Øn
            
            # 2c. Ph√¢n lo·∫°i tone (d·ª±a tr√™n interaction type distribution)
            type_counts = Counter([item["type"] for item in interactions])
            dominant_type = type_counts.most_common(1)[0][0]
            
            tone_map = {
                "debate": "analytical, argumentative, logical",
                "translation": "multilingual, literary",
                "book_analysis": "scholarly, reflective, interdisciplinary",
                "query": "curious, information-seeking"
            }
            
            # 3. C·∫≠p nh·∫≠t profile
            self.profile.update({
                "thinking_style": {
                    "favorite_keywords": common_words[:10],
                    "writing_tone": tone_map.get(dominant_type, "neutral"),
                    "debate_strategy": "First-principles reasoning"  # [Inference] Gi·∫£ ƒë·ªãnh
                },
                "interaction_history_embeddings": avg_embedding,
                "last_updated": datetime.now().isoformat()
            })
            
            # 4. L∆∞u l·∫°i DB
            profile_json = json.dumps(self.profile, ensure_ascii=False)
            self.db.table("user_profiles")\
                .upsert({
                    "user_id": self.user_id,
                    "profile_json": profile_json
                })\
                .execute()
            
            st.success("‚úÖ ƒê√£ c·∫≠p nh·∫≠t AI Profile!")
            
        except Exception as e:
            st.error(f"L·ªói update profile: {e}")
    
    def get_personalized_context(self, query, top_k=5):
        """
        L·∫•y context c√° nh√¢n h√≥a cho query
        
        Args:
            query: C√¢u h·ªèi/t√¨nh hu·ªëng hi·ªán t·∫°i
            top_k: S·ªë t∆∞∆°ng t√°c c≈© li√™n quan nh·∫•t
        
        Returns:
            str: Context ƒë·ªÉ inject v√†o prompt
        """
        if not self.profile.get("interaction_history_embeddings"):
            return ""
        
        # 1. Encode query
        query_emb = self.encoder.encode([query])[0]
        
        # 2. L·∫•y l·ªãch s·ª≠ t·ª´ DB
        try:
            response = self.db.table("user_interactions")\
                .select("*")\
                .eq("user_id", self.user_id)\
                .order("timestamp", desc=True)\
                .limit(50)\
                .execute()
            
            if not response.data:
                return ""
            
            # 3. T√≠nh similarity
            interactions = response.data
            embeddings = [json.loads(item["embedding"]) for item in interactions]
            
            similarities = cosine_similarity([query_emb], embeddings)[0]
            
            # 4. L·∫•y top_k
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            context_parts = []
            for idx in top_indices:
                item = interactions[idx]
                context_parts.append(
                    f"[{item['type']}] {item['content'][:200]}"
                )
            
            # 5. Build context string
            context = f"""
            === USER CONTEXT (Phong c√°ch t∆∞ duy c·ªßa ng∆∞·ªùi n√†y) ===
            Keywords ∆∞a th√≠ch: {', '.join(self.profile.get('thinking_style', {}).get('favorite_keywords', []))}
            Tone: {self.profile.get('thinking_style', {}).get('writing_tone', 'neutral')}
            
            === RELEVANT PAST INTERACTIONS ===
            {chr(10).join(context_parts)}
            ===
            """
            
            return context
            
        except Exception as e:
            st.warning(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c context: {e}")
            return ""
    
    def generate_persona_prompt(self):
        """
        [Inference] T·∫°o system prompt ƒë·ªÉ AI m√¥ ph·ªèng user
        
        Returns:
            str: System instruction
        """
        if not self.profile:
            return None
        
        style = self.profile.get("thinking_style", {})
        keywords = ", ".join(style.get("favorite_keywords", []))
        tone = style.get("writing_tone", "neutral")
        
        prompt = f"""
        B·∫†N ƒêANG M√î PH·ªéNG PHONG C√ÅCH T∆Ø DUY C·ª¶A USER "{self.user_id}".
        
        ƒê·∫∑c ƒëi·ªÉm:
        - T·ª´ ng·ªØ ∆∞a d√πng: {keywords}
        - Phong c√°ch vi·∫øt: {tone}
        - Chi·∫øn l∆∞·ª£c l·∫≠p lu·∫≠n: First-principles reasoning, interdisciplinary connections
        
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi THEO PHONG C√ÅCH N√ÄY, nh∆∞ th·ªÉ ch√≠nh user ƒëang t·ª± tr·∫£ l·ªùi.
        """
        
        return prompt


# === C√ÅCH T√çCH H·ª¢P V√ÄO module_weaver.py ===

def demo_personal_rag():
    """
    Th√™m v√†o TAB m·ªõi: "üß† AI H·ªçc T√¥i"
    """
    from personal_rag_system import PersonalRAG
    from ai_core import AI_Core
    
    st.header("üß† AI H·ªçc Phong C√°ch T∆∞ Duy C·ªßa B·∫°n")
    
    # Init
    if has_db:  # Bi·∫øn global t·ª´ module_weaver.py
        user_id = st.session_state.get("current_user", "Unknown")
        rag = PersonalRAG(supabase, user_id)
        ai = AI_Core()
        
        # 1. Hi·ªÉn th·ªã Profile hi·ªán t·∫°i
        with st.expander("üìä Profile hi·ªán t·∫°i"):
            st.json(rag.profile)
        
        # 2. N√∫t c·∫≠p nh·∫≠t th·ªß c√¥ng
        if st.button("üîÑ C·∫≠p nh·∫≠t Profile (ph√¢n t√≠ch l·∫°i l·ªãch s·ª≠)"):
            rag.update_profile(force=True)
        
        # 3. Demo: AI m√¥ ph·ªèng user
        st.divider()
        st.subheader("üé≠ AI M√¥ Ph·ªèng B·∫°n")
        
        test_query = st.text_area(
            "ƒê·∫∑t m·ªôt c√¢u h·ªèi ‚Üí AI s·∫Ω tr·∫£ l·ªùi THEO PHONG C√ÅCH C·ª¶A B·∫†N:",
            height=100
        )
        
        if st.button("üöÄ Ch·∫°y") and test_query:
            with st.spinner("ƒêang ph√¢n t√≠ch phong c√°ch..."):
                # L·∫•y context c√° nh√¢n h√≥a
                context = rag.get_personalized_context(test_query, top_k=3)
                
                # T·∫°o persona prompt
                persona_prompt = rag.generate_persona_prompt()
                
                # Build full prompt
                full_prompt = f"""
                {context}
                
                === NHI·ªÜM V·ª§ ===
                C√¢u h·ªèi: {test_query}
                
                H√£y tr·∫£ l·ªùi theo phong c√°ch t∆∞ duy ƒë∆∞·ª£c m√¥ t·∫£ ·ªü tr√™n.
                """
                
                # G·ªçi AI
                response = ai.generate(
                    full_prompt,
                    model_type="pro",
                    system_instruction=persona_prompt
                )
                
                st.markdown("### ü§ñ AI m√¥ ph·ªèng b·∫°n:")
                st.markdown(response)
                
                # Ghi l·∫°i interaction n√†y
                rag.record_interaction(
                    "query",
                    test_query,
                    {"ai_response": response}
                )
    else:
        st.error("C·∫ßn k·∫øt n·ªëi Supabase ƒë·ªÉ d√πng t√≠nh nƒÉng n√†y")


# === C·∫§U TR√öC TABLE SUPABASE C·∫¶N T·∫†O ===

"""
-- Table 1: user_profiles
CREATE TABLE user_profiles (
    id SERIAL PRIMARY KEY,
    user_id TEXT UNIQUE NOT NULL,
    profile_json JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Table 2: user_interactions
CREATE TABLE user_interactions (
    id SERIAL PRIMARY KEY,
    user_id TEXT NOT NULL,
    type TEXT NOT NULL,
    content TEXT NOT NULL,
    embedding JSONB NOT NULL,
    context JSONB,
    timestamp TIMESTAMP DEFAULT NOW()
);

-- Index cho t√¨m ki·∫øm nhanh
CREATE INDEX idx_user_interactions_user_id ON user_interactions(user_id);
CREATE INDEX idx_user_interactions_timestamp ON user_interactions(timestamp DESC);
"""
