"""
Weakness Analyzer
Ph√¢n t√≠ch ƒëi·ªÉm y·∫øu v√† pattern l·ªói c·ªßa h·ªçc sinh
"""
from collections import defaultdict
from typing import Dict, List, Tuple
from datetime import datetime, timedelta


class WeaknessAnalyzer:
    """
    Ph√¢n t√≠ch ƒëi·ªÉm y·∫øu d·ª±a tr√™n:
    1. Accuracy theo topic/concept
    2. Error patterns (l·ªói l·∫∑p l·∫°i)
    3. Time efficiency
    4. Confidence vs correctness gap
    5. Forgetting curve (ƒë·ªô qu√™n theo th·ªùi gian)
    """
    
    def __init__(self, history_tracker):
        """
        Args:
            history_tracker: Instance c·ªßa LearningHistoryTracker
        """
        self.tracker = history_tracker
    
    def analyze_by_topic(self) -> Dict[str, dict]:
        """
        [Inference] Ph√¢n t√≠ch performance theo topic
        
        Returns:
            {
                'topic_name': {
                    'total_attempts': int,
                    'correct': int,
                    'accuracy': float (0-1),
                    'avg_time': float (seconds),
                    'weakness_level': str,  # 'critical', 'needs_practice', 'good', 'mastered'
                    'recent_trend': str  # 'improving', 'declining', 'stable'
                }
            }
        """
        topic_stats = defaultdict(lambda: {
            'total': 0,
            'correct': 0,
            'times': [],
            'attempts_by_date': []  # ƒê·ªÉ t√≠nh trend
        })
        
        # Thu th·∫≠p data t·ª´ t·∫•t c·∫£ sessions
        for session in self.tracker.data['sessions']:
            for attempt in session['attempts']:
                topic = attempt.get('topic', 'Unknown')
                
                topic_stats[topic]['total'] += 1
                
                if attempt['is_correct']:
                    topic_stats[topic]['correct'] += 1
                
                topic_stats[topic]['times'].append(
                    attempt.get('time_spent', 0)
                )
                
                topic_stats[topic]['attempts_by_date'].append({
                    'timestamp': attempt['timestamp'],
                    'correct': attempt['is_correct']
                })
        
        # T√≠nh to√°n metrics cho m·ªói topic
        result = {}
        
        for topic, stats in topic_stats.items():
            total = stats['total']
            correct = stats['correct']
            accuracy = correct / total if total > 0 else 0
            avg_time = sum(stats['times']) / len(stats['times']) if stats['times'] else 0
            
            # Ph√¢n lo·∫°i weakness level
            weakness_level = self._classify_weakness_level(accuracy, total)
            
            # Ph√¢n t√≠ch trend (c·∫£i thi·ªán/tho√°i lui)
            recent_trend = self._analyze_trend(stats['attempts_by_date'])
            
            result[topic] = {
                'total_attempts': total,
                'correct': correct,
                'accuracy': accuracy,
                'avg_time': avg_time,
                'weakness_level': weakness_level,
                'recent_trend': recent_trend
            }
        
        return result
    
    def _classify_weakness_level(self, accuracy: float, total_attempts: int) -> str:
        """
        [Inference] Ph√¢n lo·∫°i m·ª©c ƒë·ªô y·∫øu/gi·ªèi
        
        Logic:
        - C·∫ßn √≠t nh·∫•t 3 c√¢u ƒë·ªÉ ƒë√°nh gi√°
        - < 50%: Critical (c·∫ßn t·∫≠p trung ngay)
        - 50-70%: Needs practice
        - 70-90%: Good
        - >= 90%: Mastered
        """
        if total_attempts < 3:
            return 'insufficient_data'
        
        if accuracy < 0.5:
            return 'critical'
        elif accuracy < 0.7:
            return 'needs_practice'
        elif accuracy < 0.9:
            return 'good'
        else:
            return 'mastered'
    
    def _analyze_trend(self, attempts_by_date: List[dict]) -> str:
        """
        [Inference] Ph√¢n t√≠ch xu h∆∞·ªõng ti·∫øn b·ªô
        
        So s√°nh accuracy c·ªßa 50% attempts ƒë·∫ßu vs 50% cu·ªëi
        """
        if len(attempts_by_date) < 4:
            return 'insufficient_data'
        
        # Sort theo timestamp
        attempts_by_date.sort(key=lambda x: x['timestamp'])
        
        # Chia ƒë√¥i
        mid = len(attempts_by_date) // 2
        first_half = attempts_by_date[:mid]
        second_half = attempts_by_date[mid:]
        
        # T√≠nh accuracy
        first_accuracy = sum(1 for a in first_half if a['correct']) / len(first_half)
        second_accuracy = sum(1 for a in second_half if a['correct']) / len(second_half)
        
        # So s√°nh
        diff = second_accuracy - first_accuracy
        
        if diff > 0.15:  # TƒÉng > 15%
            return 'improving'
        elif diff < -0.15:  # Gi·∫£m > 15%
            return 'declining'
        else:
            return 'stable'
    
    def find_error_patterns(self) -> List[dict]:
        """
        [Inference] T√¨m l·ªói l·∫∑p l·∫°i
        
        Ph√¢n t√≠ch:
        - C√¢u n√†o b·ªã sai nhi·ªÅu l·∫ßn
        - ƒê√°p √°n sai n√†o ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t
        - Concept n√†o hay b·ªã nh·∫ßm
        
        Returns:
            List of {
                'question_id': str,
                'question': str,
                'topic': str,
                'concept_tags': list,
                'times_wrong': int,
                'wrong_answers': list,
                'correct_answer': str,
                'last_wrong_date': str
            }
        """
        repeated_errors = []
        
        for q_id, attempts in self.tracker.data['question_bank'].items():
            # L·ªçc c√°c l·∫ßn sai
            wrong_attempts = [a for a in attempts if not a['is_correct']]
            
            if len(wrong_attempts) >= 2:  # Sai >= 2 l·∫ßn
                # Thu th·∫≠p info
                first_attempt = attempts[0]  # L·∫•y metadata t·ª´ l·∫ßn ƒë·∫ßu
                
                repeated_errors.append({
                    'question_id': q_id,
                    'question': first_attempt['question'],
                    'topic': first_attempt.get('topic', 'Unknown'),
                    'concept_tags': first_attempt.get('concept_tags', []),
                    'times_wrong': len(wrong_attempts),
                    'total_attempts': len(attempts),
                    'wrong_answers': [a['selected'] for a in wrong_attempts],
                    'correct_answer': first_attempt['correct_answer'],
                    'last_wrong_date': wrong_attempts[-1]['timestamp']
                })
        
        # S·∫Øp x·∫øp theo ƒë·ªô nghi√™m tr·ªçng (sai nhi·ªÅu nh·∫•t tr∆∞·ªõc)
        repeated_errors.sort(key=lambda x: x['times_wrong'], reverse=True)
        
        return repeated_errors
    
    def analyze_concept_confusion(self) -> Dict[str, List[str]]:
        """
        [Inference] Ph√¢n t√≠ch concept n√†o hay b·ªã nh·∫ßm l·∫´n
        
        VD: H·ªçc sinh hay nh·∫ßm "2/3" v·ªõi "3/2", ">" v·ªõi "<"
        
        Returns:
            {
                'concept_1': ['concept_2', 'concept_3'],  # Concepts hay b·ªã nh·∫ßm
                ...
            }
        """
        # TODO: C·∫ßn NLP ƒë·ªÉ extract concepts t·ª´ c√¢u h·ªèi/ƒë√°p √°n
        # Hi·ªán t·∫°i ch·ªâ d·ª±a v√†o concept_tags
        
        confusion_matrix = defaultdict(set)
        
        errors = self.find_error_patterns()
        
        for error in errors:
            concepts = error.get('concept_tags', [])
            
            # N·∫øu c√≥ >= 2 concepts trong 1 c√¢u ‚Üí C√≥ th·ªÉ b·ªã nh·∫ßm
            if len(concepts) >= 2:
                for i, c1 in enumerate(concepts):
                    for c2 in concepts[i+1:]:
                        confusion_matrix[c1].add(c2)
                        confusion_matrix[c2].add(c1)
        
        # Convert set ‚Üí list
        return {k: list(v) for k, v in confusion_matrix.items()}
    
    def get_priority_topics(self, top_n: int = 5) -> List[str]:
        """
        [Inference] Tr·∫£ v·ªÅ top N topics c·∫ßn ∆∞u ti√™n
        
        D·ª±a tr√™n:
        1. Accuracy th·∫•p
        2. S·ªë l·∫ßn sai nhi·ªÅu
        3. Trend ƒëang tho√°i lui
        
        Returns:
            List of topic names (sorted by priority)
        """
        topic_analysis = self.analyze_by_topic()
        
        # T√≠nh priority score cho m·ªói topic
        scored_topics = []
        
        for topic, data in topic_analysis.items():
            if data['weakness_level'] == 'insufficient_data':
                continue
            
            # Base score: C√†ng accuracy th·∫•p c√†ng quan tr·ªçng
            priority_score = 1 - data['accuracy']
            
            # Bonus n·∫øu ƒëang tho√°i lui
            if data['recent_trend'] == 'declining':
                priority_score += 0.3
            
            # Bonus n·∫øu l√† critical
            if data['weakness_level'] == 'critical':
                priority_score += 0.5
            
            scored_topics.append((topic, priority_score))
        
        # S·∫Øp x·∫øp theo score gi·∫£m d·∫ßn
        scored_topics.sort(key=lambda x: x[1], reverse=True)
        
        # Tr·∫£ v·ªÅ top N topic names
        return [topic for topic, score in scored_topics[:top_n]]
    
    def get_recommended_difficulty(self, topic: str) -> str:
        """
        [Inference] ƒê·ªÅ xu·∫•t ƒë·ªô kh√≥ ph√π h·ª£p cho topic
        
        D·ª±a tr√™n Zone of Proximal Development (Vygotsky):
        - Kh√¥ng qu√° d·ªÖ (boring)
        - Kh√¥ng qu√° kh√≥ (frustrating)
        
        Returns:
            'Easy üòä', 'Medium ü§î', 'Hard üò∞', 'Expert üíÄ'
        """
        topic_stats = self.analyze_by_topic()
        
        if topic not in topic_stats:
            return 'Medium ü§î'  # Default
        
        accuracy = topic_stats[topic]['accuracy']
        
        # Logic mapping
        if accuracy >= 0.9:
            return 'Hard üò∞'  # ƒê√£ gi·ªèi ‚Üí Th·ª≠ kh√≥ h∆°n
        elif accuracy >= 0.7:
            return 'Medium ü§î'  # Kh√° t·ªët ‚Üí Gi·ªØ m·ª©c v·ª´a
        elif accuracy >= 0.5:
            return 'Easy üòä'  # C√≤n y·∫øu ‚Üí D·ªÖ tr∆∞·ªõc
        else:
            return 'Easy üòä'  # R·∫•t y·∫øu ‚Üí Ph·∫£i d·ªÖ
    
    def get_spaced_repetition_schedule(self, topic: str) -> Dict[str, str]:
        """
        [Inference] T√≠nh l·ªãch √¥n t·∫≠p theo Spaced Repetition
        
        C√¥ng th·ª©c Ebbinghaus:
        - L·∫ßn 1: Sau 1 ng√†y
        - L·∫ßn 2: Sau 3 ng√†y
        - L·∫ßn 3: Sau 7 ng√†y
        - L·∫ßn 4: Sau 14 ng√†y
        
        Returns:
            {
                'next_review': str (ISO date),
                'review_count': int,
                'mastery_level': str
            }
        """
        # T√¨m l·∫ßn l√†m g·∫ßn nh·∫•t c·ªßa topic n√†y
        last_attempt = None
        
        for session in self.tracker.data['sessions']:
            for attempt in session['attempts']:
                if attempt.get('topic') == topic:
                    if not last_attempt or attempt['timestamp'] > last_attempt['timestamp']:
                        last_attempt = attempt
        
        if not last_attempt:
            return {
                'next_review': 'now',
                'review_count': 0,
                'mastery_level': 'not_started'
            }
        
        # ƒê·∫øm s·ªë l·∫ßn √¥n t·∫≠p (coi m·ªói session l√† 1 l·∫ßn)
        review_count = sum(
            1 for s in self.tracker.data['sessions']
            if any(a.get('topic') == topic for a in s['attempts'])
        )
        
        # T√≠nh ng√†y √¥n ti·∫øp theo
        last_date = datetime.fromisoformat(last_attempt['timestamp'])
        
        if review_count == 1:
            next_date = last_date + timedelta(days=1)
        elif review_count == 2:
            next_date = last_date + timedelta(days=3)
        elif review_count == 3:
            next_date = last_date + timedelta(days=7)
        else:
            next_date = last_date + timedelta(days=14)
        
        # Check mastery
        topic_stats = self.analyze_by_topic()
        mastery = topic_stats.get(topic, {}).get('weakness_level', 'unknown')
        
        return {
            'next_review': next_date.isoformat(),
            'review_count': review_count,
            'mastery_level': mastery
        }
    
    def get_time_efficiency_analysis(self) -> Dict[str, dict]:
        """
        [Inference] Ph√¢n t√≠ch hi·ªáu qu·∫£ th·ªùi gian
        
        T√¨m nh·ªØng topic n√†o:
        - M·∫•t th·ªùi gian nhi·ªÅu nh∆∞ng accuracy th·∫•p ‚Üí C·∫ßn h·ªçc l·∫°i
        - M·∫•t th·ªùi gian √≠t v√† accuracy cao ‚Üí ƒê√£ th√†nh th·∫°o
        
        Returns:
            {
                'topic': {
                    'avg_time': float,
                    'accuracy': float,
                    'efficiency_score': float,  # accuracy / time
                    'status': str  # 'efficient', 'struggling', 'needs_practice'
                }
            }
        """
        topic_stats = self.analyze_by_topic()
        
        result = {}
        
        for topic, data in topic_stats.items():
            avg_time = data['avg_time']
            accuracy = data['accuracy']
            
            # Tr√°nh chia cho 0
            if avg_time > 0:
                efficiency = accuracy / avg_time
            else:
                efficiency = 0
            
            # Ph√¢n lo·∫°i status
            if accuracy >= 0.8 and avg_time < 30:  # 30s = baseline
                status = 'efficient'
            elif accuracy < 0.6 and avg_time > 60:
                status = 'struggling'
            else:
                status = 'needs_practice'
            
            result[topic] = {
                'avg_time': avg_time,
                'accuracy': accuracy,
                'efficiency_score': efficiency,
                'status': status
            }
        
        return result
    
    def generate_study_plan(self, days: int = 7) -> List[Dict]:
        """
        [Inference] Sinh k·∫ø ho·∫°ch h·ªçc t·∫≠p cho N ng√†y t·ªõi
        
        Args:
            days: S·ªë ng√†y l√™n k·∫ø ho·∫°ch
        
        Returns:
            List of {
                'date': str,
                'topics': list,
                'focus': str,  # 'review_weak', 'spaced_repetition', 'new_material'
                'recommended_duration': int  # minutes
            }
        """
        priority_topics = self.get_priority_topics(top_n=5)
        
        plan = []
        
        for i in range(days):
            date = (datetime.now() + timedelta(days=i)).strftime('%Y-%m-%d')
            
            if i % 3 == 0:  # M·ªói 3 ng√†y: √în ƒëi·ªÉm y·∫øu
                plan.append({
                    'date': date,
                    'topics': priority_topics[:2],
                    'focus': 'review_weak',
                    'recommended_duration': 30
                })
            elif i % 3 == 1:  # Spaced repetition
                # TODO: L·∫•y topics c·∫ßn √¥n theo l·ªãch
                plan.append({
                    'date': date,
                    'topics': ['Review previous lessons'],
                    'focus': 'spaced_repetition',
                    'recommended_duration': 20
                })
            else:  # H·ªçc m·ªõi
                plan.append({
                    'date': date,
                    'topics': ['New chapter'],
                    'focus': 'new_material',
                    'recommended_duration': 40
                })
        
        return plan
