import google.generativeai as genai
import streamlit as st
import time
from google.api_core.exceptions import ResourceExhausted, ServiceUnavailable, InternalServerError, InvalidArgument

# ğŸ‘‡ IMPORT Má»šI: Config vÃ  Logger tá»« thÆ° má»¥c blocks
from services.blocks.config import AppConfig
from services.blocks.logger import AppLogger

class AI_Core:
    def __init__(self):
        self.logger = AppLogger() # âœ… Khá»Ÿi táº¡o Logger
        self.api_ready = False
        try:
            # Kiá»ƒm tra key tá»“n táº¡i trÆ°á»›c khi láº¥y
            if "api_keys" in st.secrets and "gemini_api_key" in st.secrets["api_keys"]:
                api_key = st.secrets["api_keys"]["gemini_api_key"]
                genai.configure(api_key=api_key)
                self.api_ready = True
            else:
                st.error("âš ï¸ ChÆ°a cáº¥u hÃ¬nh API Key trong secrets.toml")
                self.logger.log_error("AI_Core", "Missing API Key", "") # âœ… Log lá»—i
                return

            # Cáº¥u hÃ¬nh Safety (Cháº·n ná»™i dung Ä‘á»™c háº¡i)
            self.safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            # Cáº¥u hÃ¬nh Quota Monitor (Theo yÃªu cáº§u B)
            self.quota_tracker = {
                "daily_calls": 0,
                "daily_limit": AppConfig.API_LIMITS["gemini_daily"], # âœ… Láº¥y tá»« Config
                "cost_estimate": 0.0
            }

        except Exception as e:
            st.error(f"âŒ Lá»—i khá»Ÿi táº¡o AI Core: {e}")
            self.logger.log_error("AI_Core_Init", str(e), "") # âœ… Log lá»—i

    # Cáº¥u hÃ¬nh Generation Config (Theo yÃªu cáº§u A)
    def _get_gen_config(self, task_type="general"):
        configs = {
            "debate": {"temperature": 0.9, "max_tokens": 2048},
            "translation": {"temperature": 0.3, "max_tokens": 4096},
            "book_analysis": {"temperature": 0.7, "max_tokens": 8192},
            "general": {"temperature": 0.8, "max_tokens": 1024}
        }
        return genai.GenerationConfig(**configs.get(task_type, configs["general"]))

    # Track usage (Theo yÃªu cáº§u B)
    def _track_usage(self, model_name, tokens_used):
        """Track API usage Ä‘á»ƒ trÃ¡nh vÆ°á»£t quota"""
        self.quota_tracker["daily_calls"] += 1
        
        # Gemini pricing (example)
        cost_per_1k = {
            "gemini-2.5-pro": 0.0035,
            "gemini-2.5-flash": 0.00035
        }
        self.quota_tracker["cost_estimate"] += (tokens_used / 1000) * cost_per_1k.get(model_name, 0)
        
        if self.quota_tracker["daily_calls"] > self.quota_tracker["daily_limit"]:
            st.warning(f"âš ï¸ ÄÃ£ sá»­ dá»¥ng {self.quota_tracker['daily_calls']} API calls hÃ´m nay!")

    def _get_model(self, model_name, system_instr=None, task_type="general"):
        """HÃ m helper Ä‘á»ƒ khá»Ÿi táº¡o model Ä‘Ãºng phiÃªn báº£n"""
        # âœ… DANH SÃCH MODEL Tá»ª CONFIG
        valid_names = AppConfig.GEMINI_MODELS
        
        # Máº·c Ä‘á»‹nh fallback vá» 2.5 Flash náº¿u tÃªn sai
        target_name = valid_names.get(model_name, valid_names["flash"])
        
        try:
            return genai.GenerativeModel(
                model_name=target_name,
                safety_settings=self.safety_settings,
                generation_config=self._get_gen_config(task_type), # Cáº­p nháº­t dÃ¹ng config Ä‘á»™ng
                system_instruction=system_instr
            )
        except Exception as e:
            # st.warning(f"âš ï¸ KhÃ´ng thá»ƒ khá»Ÿi táº¡o model {target_name}: {e}")
            return None

    def generate(self, prompt, model_type="flash", system_instruction=None, task_type="general"):
        """
        HÃ m gá»i AI chÃ­nh: Tá»± Ä‘á»™ng chuyá»ƒn model náº¿u lá»—i (Fallback Strategy)
        """
        start_time = time.time() # âœ… Báº¯t Ä‘áº§u Ä‘o thá»i gian

        if not self.api_ready:
            return "âš ï¸ API Key chÆ°a sáºµn sÃ ng."

        # âœ… CHIáº¾N THUáº¬T Æ¯U TIÃŠN: Pro -> Flash -> Exp
        if model_type == "pro":
            # Vá»›i task khÃ³ (Tranh biá»‡n): Æ¯u tiÃªn 2.5 Pro
            plan = [
                ("pro", "Gemini 2.5 pro", 6), 
                ("flash", "Gemini 2.5 Flash", 3), 
                ("exp", "gemini-2.5-flash-lite", 3)
            ]
        else:
            # Vá»›i task thÆ°á»ng: Æ¯u tiÃªn Flash cho nhanh
            plan = [
                ("flash", "Gemini 2.5 Flash", 2), 
                ("exp", "gemini-2.5-flash-lite", 2),
                ("pro", "Gemini 2.5 Pro", 6)
            ]

        last_errors = []
        quota_exhausted_count = 0

        for m_type, m_name, base_wait_time in plan:
            try:
                # Khá»Ÿi táº¡o model
                model = self._get_model(m_type, system_instr=system_instruction, task_type=task_type)
                if not model: continue
                
                # Gá»i API
                response = model.generate_content(prompt)
                
                # Kiá»ƒm tra káº¿t quáº£
                if response and hasattr(response, 'text') and response.text:
                    # Tracking usage khi thÃ nh cÃ´ng
                    token_count = 0
                    if hasattr(response, 'usage_metadata'):
                        token_count = response.usage_metadata.total_token_count
                    self._track_usage(m_name, token_count)
                    
                    # âœ… Log thÃ nh cÃ´ng
                    latency = time.time() - start_time
                    self.logger.log_api_call(m_type, token_count or len(prompt), latency, True)
                    
                    return response.text
                
                # Xá»­ lÃ½ cÃ¡c lÃ½ do bá»‹ cháº·n (Safety, Token...)
                if response and hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        reason = candidate.finish_reason.name
                        if reason == "SAFETY":
                            last_errors.append(f"{m_name}: Bá»‹ cháº·n (Safety)")
                            continue
                        elif reason == "MAX_TOKENS":
                            last_errors.append(f"{m_name}: QuÃ¡ dÃ i (Max Tokens)")
                            continue
                
                last_errors.append(f"{m_name}: Tráº£ vá» rá»—ng")
                continue
            
            except ResourceExhausted:
                # Lá»—i háº¿t tiá»n/quota -> Chá» lÃ¢u hÆ¡n má»™t chÃºt rá»“i thá»­ model khÃ¡c
                quota_exhausted_count += 1
                error_msg = f"{m_name}: Háº¿t Quota (429)"
                last_errors.append(error_msg)
                self.logger.log_error("Generate", error_msg, "") # âœ… Log lá»—i
                time.sleep(base_wait_time * quota_exhausted_count)
                
            except (ServiceUnavailable, InternalServerError):
                # Lá»—i Server Google -> Chá» ngáº¯n
                last_errors.append(f"{m_name}: Lá»—i Server (5xx)")
                time.sleep(2)
            
            except InvalidArgument as e:
                # Lá»—i Input -> Dá»«ng luÃ´n, khÃ´ng thá»­ láº¡i
                self.logger.log_error("Generate", f"Invalid Argument: {str(e)}", "") # âœ… Log lá»—i
                return f"âš ï¸ Lá»—i Input (Prompt khÃ´ng há»£p lá»‡): {str(e)[:200]}"
                
            except Exception as e:
                last_errors.append(f"{m_name}: Lá»—i láº¡ ({str(e)[:50]})")
                self.logger.log_error("Generate", f"Exception: {str(e)}", "") # âœ… Log lá»—i
                time.sleep(1)

        # Náº¿u thá»­ háº¿t cÃ¡c model mÃ  váº«n lá»—i
        error_summary = "\n".join(f"- {e}" for e in last_errors[-3:])

        # --- NEW: Log detailed last_errors for debugging ---
        try:
            if hasattr(self, "logger"):
                self.logger.log_error("Generate_Final_Errors", error_summary, str(last_errors))
        except Exception:
            pass

        return f"âš ï¸ Há»‡ thá»‘ng Ä‘ang báº­n hoáº·c gáº·p lá»—i:\n{error_summary}\n\nğŸ’¡ Vui lÃ²ng thá»­ láº¡i sau 1 phÃºt."láº¡i sau 1 phÃºt."

    @staticmethod
    @st.cache_data(show_spinner=False, ttl=3600)
    def analyze_static(text_hash, text, instruction):
        """
        text_hash = hashlib.md5(text.encode()).hexdigest()
        Chá»‰ cache theo hash, tiáº¿t kiá»‡m bá»™ nhá»›
        HÃ m dÃ¹ng riÃªng cho RAG (Äá»c tÃ i liá»‡u) - CÃ³ Cache Ä‘á»ƒ tiáº¿t kiá»‡m tiá»n
        """
        try:
            api_key = st.secrets["api_keys"]["gemini_api_key"]
            genai.configure(api_key=api_key)
            
            # LuÃ´n dÃ¹ng Flash cho RAG vÃ¬ nÃ³ Ä‘á»c context dÃ i tá»‘t vÃ  ráº»
            model = genai.GenerativeModel(
                "gemini-2.5-flash",
                system_instruction=instruction
            )
            
            # Cáº¯t bá»›t náº¿u text quÃ¡ dÃ i (trÃ¡nh lá»—i quÃ¡ táº£i)
            max_chars = 200000 
            truncated_text = text[:max_chars]
            
            if len(text) > max_chars:
                st.warning(f"âš ï¸ TÃ i liá»‡u quÃ¡ dÃ i, chá»‰ phÃ¢n tÃ­ch {max_chars:,} kÃ½ tá»± Ä‘áº§u.")
            
            response = model.generate_content(truncated_text)
            
            if response and hasattr(response, 'text') and response.text:
                return response.text
            else:
                return "âš ï¸ KhÃ´ng cÃ³ pháº£n há»“i tá»« AI."
                
        except Exception as e:
            return f"âŒ Lá»—i phÃ¢n tÃ­ch tÄ©nh: {str(e)[:200]}"
