"""
Module: module_weaver.py
Clean, robust rewrite of the Cognitive Weaver UI module (Streamlit).
Key points:
- Use services.blocks imports
- Use ServiceLocator to obtain singletons (ai_core, voice)
- Robust AI calls with retry/fallback and logging
- Avoid appending raw API error texts into chat history (prevents false consensus)
- Consensus detection ignores system-error notes
"""
import time
from datetime import datetime
import json
import streamlit as st
import pandas as pd
import numpy as np

# Blocks imports (explicit service paths)
from services.blocks.file_processor import doc_file
from services.blocks.embedding_engine import load_encoder
from services.blocks.html_generator import create_html_block, create_interactive_html_block
from services.blocks.rag_orchestrator import (
    analyze_document_streamlit,
    compute_similarity_with_excel,
    store_history,
    init_knowledge_universe,
    tai_lich_su,
)
from services.blocks.prompts import DEBATE_PERSONAS, BOOK_ANALYSIS_PROMPT
from services.blocks.service_locator import ServiceLocator
from services.blocks.argument_analyzer import ArgumentAnalyzer
from services.blocks.reading_tracker import ReadingProgressTracker

# Optional supabase import (don't fail app if missing)
try:
    from supabase import create_client, Client
except Exception:
    create_client = None

# UI Translations
TRANS = {
    "vi": {
        "lang_select": "NgÃ´n ngá»¯ / Language / è¯­è¨€",
        "tab1": "ğŸ“š PhÃ¢n TÃ­ch SÃ¡ch",
        "tab2": "âœï¸ Dá»‹ch Giáº£",
        "tab3": "ğŸ—£ï¸ Tranh Biá»‡n",
        "tab4": "ğŸ™ï¸ PhÃ²ng Thu AI",
        "tab5": "â³ Nháº­t KÃ½",
        "t1_header": "Trá»£ lÃ½ NghiÃªn cá»©u & Knowledge Graph",
        "t1_up_excel": "1. Káº¿t ná»‘i Kho SÃ¡ch (Excel)",
        "t1_up_doc": "2. TÃ i liá»‡u má»›i (PDF/Docx)",
        "t1_btn": "ğŸš€ PHÃ‚N TÃCH NGAY",
        "t1_analyzing": "Äang phÃ¢n tÃ­ch {name}...",
        "t1_connect_ok": "âœ… ÄÃ£ káº¿t ná»‘i {n} cuá»‘n sÃ¡ch.",
        "t1_graph_title": "ğŸª VÅ© trá»¥ SÃ¡ch",
        "t2_header": "Dá»‹ch Thuáº­t Äa Chiá»u",
        "t2_input": "Nháº­p vÄƒn báº£n cáº§n dá»‹ch:",
        "t2_btn": "âœï¸ Dá»‹ch Ngay",
        "t3_header": "Äáº¥u TrÆ°á»ng TÆ° Duy",
        "t3_persona_label": "Chá»n Äá»‘i Thá»§:",
        "t3_input": "Nháº­p chá»§ Ä‘á» tranh luáº­n...",
        "t3_clear": "ğŸ—‘ï¸ XÃ³a Chat",
        "t4_header": "ğŸ™ï¸ PhÃ²ng Thu AI Äa NgÃ´n Ngá»¯",
        "t4_btn": "ğŸ”Š Táº O AUDIO",
        "t5_header": "Nháº­t KÃ½ & Lá»‹ch Sá»­",
        "t5_refresh": "ğŸ”„ Táº£i láº¡i Lá»‹ch sá»­",
        "t5_empty": "ChÆ°a cÃ³ dá»¯ liá»‡u lá»‹ch sá»­.",
    },
    "en": {
        "lang_select": "Language",
        "tab1": "ğŸ“š Book Analysis",
        "tab2": "âœï¸ Translator",
        "tab3": "ğŸ—£ï¸ Debater",
        "tab4": "ğŸ™ï¸ AI Studio",
        "tab5": "â³ History",
    },
    "zh": {
        "lang_select": "è¯­è¨€",
        "tab1": "ğŸ“š ä¹¦ç±åˆ†æ",
        "tab2": "âœï¸ ç¿»è¯‘ä¸“å®¶",
        "tab3": "ğŸ—£ï¸ è¾©è®ºåœº",
        "tab4": "ğŸ™ï¸ AI å½•éŸ³å®¤",
        "tab5": "â³ å†å²è®°å½•",
    }
}


def T(key):
    lang = st.session_state.get('weaver_lang', 'vi')
    return TRANS.get(lang, TRANS['vi']).get(key, key)


@st.cache_resource
def load_models():
    try:
        return load_encoder()
    except Exception:
        return None


def get_knowledge_universe():
    ku = st.session_state.get("knowledge_universe", None)
    if ku is not None:
        return ku
    try:
        ku = init_knowledge_universe()
        st.session_state["knowledge_universe"] = ku
        return ku
    except Exception:
        return None


# -----------------------
# Helper: persona generation with retry/fallback
# -----------------------
def _persona_generate_with_retry(ai_instance, prompt, persona_name, initial_model="pro", max_attempts=3, short_context_limit=800):
    """
    Try to generate persona response with retries and fallbacks.
    Returns (ok: bool, response: str, error_summary: str)
    - ok True when we obtained a usable response
    - response: AI text if ok else ""
    - error_summary: final error message (for logging)
    """
    error_markers = ["Há»‡ thá»‘ng Ä‘ang báº­n", "[System Busy", "âš ï¸ Há»‡ thá»‘ng", "[API Error", "Lá»—i", "System Busy", "exhausted"]
    attempt = 0
    last_err = ""
    model_plan = [initial_model, "flash", "flash"]

    while attempt < max_attempts:
        model_choice = model_plan[min(attempt, len(model_plan) - 1)]
        try:
            res = ai_instance.generate(prompt, model_type=model_choice, system_instruction=DEBATE_PERSONAS.get(persona_name))
        except Exception as e:
            res = f"âš ï¸ Lá»—i khi gá»i AI: {e}"

        # If response looks like system busy / error -> retry
        if not res or any(marker in res for marker in error_markers):
            last_err = res
            # log attempt if logger available
            try:
                if hasattr(ai_instance, "logger"):
                    ai_instance.logger.log_error("Persona_Generate_Attempt", f"{persona_name} attempt={attempt+1} model={model_choice}", str(res))
            except Exception:
                pass

            time.sleep(0.8 * (attempt + 1))
            # Try to shorten prompt after second failure
            if attempt == 1 and len(prompt) > short_context_limit:
                prompt = prompt[-short_context_limit:]
            attempt += 1
            continue

        return True, res, ""
    return False, "", last_err


# -----------------------
# Helper: consensus detection (ignore system-error notes)
# -----------------------
def _check_consensus_reached(chat_history):
    if len(chat_history) < 4:
        return False
    last_two = [chat_history[-2]['content'], chat_history[-1]['content']]
    error_markers = ["Há»‡ thá»‘ng Ä‘ang báº­n", "[System Busy", "âš ï¸ Há»‡ thá»‘ng", "[API Error", "Lá»—i", "System Busy", "exhausted"]
    if any(any(marker in s for marker in error_markers) for s in last_two):
        return False

    # try to use encoder similarity
    encoder = load_models()
    if encoder is not None:
        try:
            embs = encoder.encode(last_two)
            from sklearn.metrics.pairwise import cosine_similarity
            sim = cosine_similarity([embs[0]], [embs[1]])[0][0]
            if sim > 0.85:
                return True
        except Exception:
            pass

    agreement_keywords = ["Ä‘á»“ng Ã½", "Ä‘Ãºng", "thá»«a nháº­n", "agree", "correct", "nháº¥t trÃ­", "thá»‘ng nháº¥t"]
    last_msg = chat_history[-1]['content'].lower()
    if any(kw in last_msg for kw in agreement_keywords):
        return True
    return False


# -----------------------
# Simple KG helper used in UI (kept small)
# -----------------------
def find_related_books_with_decay(ku, query_text, top_k=3):
    if not ku or not hasattr(ku, 'graph'):
        return []
    encoder = load_models()
    if not encoder:
        return []
    try:
        query_emb = encoder.encode([query_text])[0]
    except Exception:
        return []

    scored_nodes = []
    current_time = datetime.now()
    for node_id in ku.graph.nodes:
        node = ku.graph.nodes[node_id]
        if "embedding" not in node:
            continue
        other_emb = node["embedding"]
        # cosine similarity safe
        sim = float(np.dot(query_emb, other_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(other_emb) + 1e-9))
        # time decay
        time_factor = 1.0
        added_at = node.get("added_at")
        if added_at:
            try:
                days_old = (current_time - datetime.fromisoformat(added_at)).days
                if days_old < 0: days_old = 0
                time_factor = np.exp(-0.001 * days_old)
            except Exception:
                pass
        score = sim * time_factor
        scored_nodes.append((node_id, score))
    scored_nodes.sort(key=lambda x: x[1], reverse=True)
    results = []
    for nid, score in scored_nodes[:top_k]:
        title = ku.graph.nodes[nid].get("title", nid)
        explanation = ku.graph.nodes[nid].get("summary", "")[:120] + "..."
        results.append((nid, title, score, explanation))
    return results


# -----------------------
# Main Run (Streamlit)
# -----------------------
def run():
    # Acquire services
    ai = ServiceLocator.get("ai_core")
    voice = ServiceLocator.get("voice_engine")

    # Ensure session state defaults
    if 'weaver_lang' not in st.session_state:
        st.session_state.weaver_lang = 'vi'
    if 'weaver_chat' not in st.session_state:
        st.session_state.weaver_chat = []

    knowledge_universe = get_knowledge_universe()

    # Sidebar
    with st.sidebar:
        st.markdown("---")
        lang_choice = st.selectbox("ğŸŒ " + T("lang_select"), ["Tiáº¿ng Viá»‡t", "English", "ä¸­æ–‡"], key="weaver_lang_selector")
        if lang_choice == "Tiáº¿ng Viá»‡t":
            st.session_state.weaver_lang = 'vi'
        elif lang_choice == "English":
            st.session_state.weaver_lang = 'en'
        else:
            st.session_state.weaver_lang = 'zh'
        st.divider()
        if st.button("ğŸ”„ Clear Chat History"):
            st.session_state.weaver_chat = []
            st.experimental_rerun()

    st.header("ğŸ§  The Cognitive Weaver")

    # Tabs
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
        [T("tab1"), T("tab2"), T("tab3"), T("tab4"), T("tab5"), "ğŸ“– Reading Tracker"]
    )

    # ---------------- Tab 1: RAG / Document analysis ----------------
    with tab1:
        st.header(T("t1_header"))
        c1, c2, c3 = st.columns([1, 1, 1])
        with c1:
            file_excel = st.file_uploader(T("t1_up_excel"), type="xlsx", key="t1_excel")
        with c2:
            uploaded_files = st.file_uploader(T("t1_up_doc"), type=["pdf", "docx", "txt", "md", "html"], accept_multiple_files=True)
        with c3:
            st.write("")
            btn_run = st.button(T("t1_btn"), type="primary", use_container_width=True)

        if btn_run and uploaded_files:
            vec = load_models()
            db_df = None
            if file_excel:
                try:
                    db_df = pd.read_excel(file_excel).dropna(subset=["TÃªn sÃ¡ch"])
                    st.success(T("t1_connect_ok").format(n=len(db_df)))
                except Exception as e:
                    st.error(f"âŒ Lá»—i Ä‘á»c Excel: {e}")

            for f in uploaded_files:
                text = doc_file(f)
                if not text:
                    st.warning(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file {f.name}")
                    continue

                # compute similarity to excel DB if available
                link = ""
                if db_df is not None and vec is not None:
                    try:
                        matches = compute_similarity_with_excel(text, db_df, vec)
                        if matches:
                            link = "\n".join([f"- {m[0]} ({m[1]*100:.0f}%)" for m in matches])
                    except Exception:
                        link = ""

                # knowledge graph related
                related = []
                if knowledge_universe:
                    try:
                        related = find_related_books_with_decay(knowledge_universe, text[:2000], top_k=3)
                    except Exception:
                        related = []

                with st.spinner(T("t1_analyzing").format(name=f.name)):
                    res = analyze_document_streamlit(f.name, text, user_lang=st.session_state.get('weaver_lang', 'vi'))
                    if res and "Lá»—i" not in res and not str(res).startswith("âš ï¸ Há»‡ thá»‘ng Ä‘ang báº­n"):
                        st.markdown(f"### ğŸ“„ {f.name}")
                        if link:
                            st.markdown("**SÃ¡ch cÃ³ liÃªn quan (tá»« Excel):**")
                            st.markdown(link)
                        st.markdown(res)
                        if related:
                            st.markdown("**SÃ¡ch liÃªn quan tá»« Knowledge Graph:**")
                            for node_id, title, score, explanation in related:
                                st.markdown(f"- **{title}** (Relevance: {score:.3f}) â€” {explanation}")
                        st.markdown("---")
                        try:
                            store_history("PhÃ¢n TÃ­ch SÃ¡ch", f.name, str(res)[:500])
                        except Exception:
                            pass
                    else:
                        st.error(f"âŒ KhÃ´ng thá»ƒ phÃ¢n tÃ­ch file {f.name}: {res}")

    # ---------------- Tab 2: Simple Translator (fallback) ----------------
    with tab2:
        st.subheader(T("t2_header"))
        txt = st.text_area(T("t2_input"), height=150, key="weaver_trans_inp")
        if st.button(T("t2_btn")) and txt:
            if not ai:
                st.error("AI service chÆ°a sáºµn sÃ ng.")
            else:
                style = "Default"
                prompt = f"Translate to Vietnamese. Style: {style}. Text: {txt}"
                try:
                    # simple call with fallback model selection handled inside ai.generate
                    out = ai.generate(prompt, model_type="pro")
                except Exception as e:
                    out = f"âš ï¸ Lá»—i khi gá»i AI: {e}"
                st.markdown(out)
                try:
                    store_history("Dá»‹ch Thuáº­t", "Translate", txt[:200])
                except Exception:
                    pass

    # ---------------- Tab 3: Debates (Solo & Multi-Agent) ----------------
    with tab3:
        st.subheader(T("t3_header"))
        mode = st.radio("Mode:", ["ğŸ‘¤ Solo", "âš”ï¸ Multi-Agent"], horizontal=True, key="weaver_debate_mode")
        # display chat history
        for msg in st.session_state.weaver_chat:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            st.chat_message(role).write(content)

        if mode == "ğŸ‘¤ Solo":
            col1, col2 = st.columns([3, 1])
            with col1:
                persona = st.selectbox(T("t3_persona_label"), list(DEBATE_PERSONAS.keys()), key="weaver_solo_persona")
            with col2:
                if st.button(T("t3_clear")):
                    st.session_state.weaver_chat = []
                    st.experimental_rerun()

            if prompt := st.chat_input(T("t3_input")):
                st.chat_message("user").write(prompt)
                st.session_state.weaver_chat.append({"role": "user", "content": prompt})
                recent = st.session_state.weaver_chat[-10:]
                ctx = "\n".join([f"{m.get('role','').upper()}: {m.get('content','')}" for m in recent])
                full_prompt = f"Lá»ŠCH Sá»¬:\n{ctx}\n\nNHIá»†M Vá»¤: Tráº£ lá»i cÃ¢u há»i má»›i nháº¥t cá»§a USER."
                # Use retry helper
                if not ai:
                    st.warning("AI service chÆ°a sáºµn sÃ ng.")
                else:
                    ok, res_text, err_text = _persona_generate_with_retry(ai, full_prompt, persona, initial_model="flash", max_attempts=2)
                    if not ok:
                        st.warning(f"AI tráº£ vá» lá»—i cho persona {persona}: {err_text}")
                        note = f"(AI lá»—i cho {persona} - Ä‘Ã£ bá» qua. {datetime.now().strftime('%H:%M:%S')})"
                        st.session_state.weaver_chat.append({"role": "assistant", "content": note})
                    else:
                        st.chat_message("assistant").write(res_text)
                        st.session_state.weaver_chat.append({"role": "assistant", "content": res_text})
                        try:
                            store_history("Tranh Biá»‡n Solo", f"{persona}", f"Q: {prompt}\nA: {res_text}")
                        except Exception:
                            pass

        else:
            participants = st.multiselect("Chá»n Há»™i Äá»“ng:", list(DEBATE_PERSONAS.keys()),
                                          default=list(DEBATE_PERSONAS.keys())[:2], max_selections=3)
            topic = st.text_input("Chá»§ Ä‘á»:", key="weaver_multi_topic")
            if st.button("ğŸ”¥ KHAI CHIáº¾N", disabled=(len(participants) < 2 or not topic)):
                st.session_state.weaver_chat = []
                start_msg = f"ğŸ“¢ **CHá»¦ Tá»ŒA:** Khai máº¡c tranh luáº­n vá»: *'{topic}'*"
                st.session_state.weaver_chat.append({"role": "system", "content": start_msg})
                st.info(start_msg)
                full_transcript = [start_msg]
                MAX_DEBATE_TIME = 600
                start_time = time.time()

                with st.status("ğŸ”¥ Cuá»™c chiáº¿n Ä‘ang diá»…n ra (3 vÃ²ng)...") as status:
                    try:
                        for round_num in range(1, 4):
                            if time.time() - start_time > MAX_DEBATE_TIME:
                                st.warning("â° Háº¿t giá»! Cuá»™c tranh luáº­n káº¿t thÃºc sá»›m.")
                                break
                            status.update(label=f"ğŸ”„ VÃ²ng {round_num}/3 Ä‘ang diá»…n ra...")

                            for p_name in participants:
                                # build context
                                recent_msgs = st.session_state.weaver_chat[-6:]
                                context_str = "\n".join([f"{m.get('role','')}: {m.get('content','')}" for m in recent_msgs]) or topic
                                if round_num == 1:
                                    p_prompt = f"CHá»¦ Äá»€: {topic}\nNHIá»†M Vá»¤ (VÃ²ng 1 - Má»Ÿ Ä‘áº§u): NÃªu quan Ä‘iá»ƒm chÃ­nh vÃ  2-3 lÃ½ láº½."
                                else:
                                    p_prompt = f"CHá»¦ Äá»€: {topic}\nBá»I Cáº¢NH:\n{context_str}\n\nNHIá»†M Vá»¤ (VÃ²ng {round_num} - Pháº£n biá»‡n): Pháº£n biá»‡n sáº¯c bÃ©n vÃ  cá»§ng cá»‘ láº­p trÆ°á»ng."

                                if not ai:
                                    short_note = f"(AI khÃ´ng sáºµn sÃ ng cho {p_name})"
                                    st.session_state.weaver_chat.append({"role": "assistant", "content": short_note})
                                    full_transcript.append(short_note)
                                    continue

                                ok, res_text, err_text = _persona_generate_with_retry(ai, p_prompt, p_name, initial_model="pro", max_attempts=3)
                                if not ok:
                                    st.warning(f"AI tráº£ vá» lá»—i cho {p_name}: {err_text}")
                                    short_note = f"(AI lá»—i cho {p_name} - vÃ²ng {round_num} - {datetime.now().strftime('%H:%M:%S')})"
                                    st.session_state.weaver_chat.append({"role": "assistant", "content": short_note})
                                    full_transcript.append(short_note)
                                    # log final failure if logger available
                                    try:
                                        if hasattr(ai, "logger"):
                                            ai.logger.log_error("Persona_Failure", f"{p_name} round={round_num}", err_text)
                                    except Exception:
                                        pass
                                    continue

                                # normal flow
                                clean_res = res_text.replace(f"{p_name}:", "").strip()
                                icon_map = {"Káº» Pháº£n Biá»‡n": "ğŸ˜ˆ", "Shushu": "ğŸ©", "Pháº­t Tá»•": "ğŸ™", "Socrates": "ğŸ›ï¸"}
                                icon = icon_map.get(p_name, "ğŸ¤–")
                                content_fmt = f"### {icon} {p_name}\n\n{clean_res}"
                                st.session_state.weaver_chat.append({"role": "assistant", "content": content_fmt})
                                full_transcript.append(content_fmt)
                                with st.chat_message("assistant", avatar=icon):
                                    st.markdown(content_fmt)
                                time.sleep(0.6)

                            # check consensus after each round
                            if _check_consensus_reached(st.session_state.weaver_chat):
                                status.update(label="âœ… Tranh luáº­n Ä‘Ã£ Ä‘áº¡t Ä‘á»“ng thuáº­n!", state="complete")
                                st.info("âœ… CÃ¡c bÃªn Ä‘Ã£ tÃ¬m tháº¥y Ä‘iá»ƒm chung (Consensus Reached). Dá»«ng tranh luáº­n.")
                                break

                        status.update(label="âœ… Tranh luáº­n káº¿t thÃºc!", state="complete")
                    except Exception as e:
                        st.error(f"Lá»—i trong quÃ¡ trÃ¬nh tranh luáº­n: {e}")

                # store full transcript
                full_log = "\n\n".join(full_transcript)
                try:
                    store_history("Há»™i Äá»“ng Tranh Biá»‡n", f"Chá»§ Ä‘á»: {topic}", full_log)
                except Exception:
                    pass

        # Analysis & fallacy checker area (always visible)
        st.divider()
        st.markdown("### ğŸ§  PhÃ¢n TÃ­ch Logic & Ngá»¥y Biá»‡n")
        arg_text = st.text_area("Nháº­p Ä‘oáº¡n láº­p luáº­n cáº§n kiá»ƒm tra:", height=100, key="weaver_arg_inp")
        if st.button("ğŸ” PhÃ¢n tÃ­ch Láº­p luáº­n", key="weaver_arg_btn"):
            ana = ArgumentAnalyzer()
            res = ana.analyze_argument(arg_text)
            st.metric("Äiá»ƒm Logic", f"{res['strength']}/100")
            if res['fallacies']:
                st.error("âš ï¸ PhÃ¡t hiá»‡n Ngá»¥y biá»‡n:")
                for f in res['fallacies']:
                    st.write(f"- **{f['type']}**: {f['explanation']}")
            else:
                st.success("âœ… Láº­p luáº­n vá»¯ng cháº¯c.")

    # ---------------- Tab 4: Voice / TTS ----------------
    with tab4:
        st.subheader(T("t4_header"))
        text_v = st.text_area("Text for TTS:", height=160)
        if st.button(T("t4_btn")) and text_v:
            if not voice:
                st.error("Voice engine chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o.")
            else:
                path = voice.speak(text_v)
                if path:
                    st.audio(path)

    # ---------------- Tab 5: History / Bayes ----------------
    with tab5:
        st.subheader("â³ Nháº­t KÃ½ & Pháº£n Chiáº¿u TÆ° Duy")
        if st.button("ğŸ”„ Táº£i láº¡i", key="weaver_hist_refresh"):
            st.session_state.history_cloud = tai_lich_su()
            st.experimental_rerun()
        data = st.session_state.get("history_cloud", tai_lich_su())
        if data:
            df_h = pd.DataFrame(data)
            if "SentimentScore" in df_h.columns:
                try:
                    import plotly.express as px
                    df_h["score"] = pd.to_numeric(df_h.get("SentimentScore", 0), errors='coerce').fillna(0)
                    fig = px.line(df_h, x="Time", y="score", markers=True)
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    pass

            with st.expander("ğŸ”® PhÃ¢n tÃ­ch TÆ° duy theo xÃ¡c suáº¥t Bayes (E.T. Jaynes)", expanded=False):
                st.info("AI sáº½ coi nháº­t kÃ½ hoáº¡t Ä‘á»™ng lÃ  dá»¯ liá»‡u quan sÃ¡t Ä‘á»ƒ phÃ¢n tÃ­ch tÆ° duy.")
                if st.button("ğŸ§  Cháº¡y MÃ´ hÃ¬nh Bayes ngay"):
                    recent_logs = df_h.tail(10).to_dict(orient="records")
                    prompt = f"PhÃ¢n tÃ­ch chuá»—i nháº­t kÃ½ sau nhÆ° bÃ i toÃ¡n Bayes:\n{json.dumps(recent_logs, ensure_ascii=False)}"
                    if ai:
                        try:
                            analysis = ai.generate(prompt, model_type="pro")
                        except Exception as e:
                            analysis = f"âš ï¸ Lá»—i khi gá»i AI: {e}"
                        st.markdown(analysis)
                    else:
                        st.error("AI khÃ´ng sáºµn sÃ ng.")

            for idx, item in df_h.iterrows():
                t = str(item.get('Time', ''))
                tp = str(item.get('Type', ''))
                ti = str(item.get('Title', ''))
                ct = str(item.get('Content', ''))
                icon = "ğŸ“"
                if "Tranh Biá»‡n" in tp:
                    icon = "ğŸ—£ï¸"
                elif "Dá»‹ch" in tp:
                    icon = "âœï¸"
                with st.expander(f"{icon} {t} | {tp} | {ti}"):
                    st.markdown(ct)
        else:
            st.info(T("t5_empty"))

    # ---------------- Tab 6: Reading Tracker ----------------
    with tab6:
        st.subheader("ğŸ“Š Tiáº¿n Ä‘á»™ Ä‘á»c sÃ¡ch & Spaced Repetition")
        if "current_user" in st.session_state and st.session_state.current_user:
            if create_client:
                try:
                    url = st.secrets["supabase"]["url"]
                    key = st.secrets["supabase"]["key"]
                    db_client = create_client(url, key)
                    tracker = ReadingProgressTracker(db_client, st.session_state.current_user)
                    due = tracker.get_due_reviews()
                    if due:
                        st.warning(f"â° {len(due)} sÃ¡ch cáº§n Ã´n táº­p!")
                        for rev in due:
                            book_title = "SÃ¡ch"
                            if isinstance(rev.get('reading_progress'), dict):
                                book_title = rev['reading_progress'].get('book_title', 'SÃ¡ch')
                            with st.expander(f"ğŸ“˜ {book_title} (Láº§n {rev.get('repetition',0)})"):
                                q = st.slider("Äá»™ nhá»› (0-5):", 0, 5, key=f"q_{rev.get('book_id')}")
                                if st.button("LÆ°u Ä‘Ã¡nh giÃ¡", key=f"b_{rev.get('book_id')}"):
                                    tracker.review_book(rev.get('book_id'), q)
                                    st.success("ÄÃ£ lÆ°u!")
                                    time.sleep(1)
                                    st.experimental_rerun()
                    else:
                        st.success("âœ… Báº¡n Ä‘Ã£ hoÃ n thÃ nh bÃ i Ã´n táº­p hÃ´m nay.")
                except Exception as e:
                    st.error(f"Lá»—i káº¿t ná»‘i DB: {e}")
            else:
                st.info("Supabase client chÆ°a cáº¥u hÃ¬nh; khÃ´ng thá»ƒ truy váº¥n Reading Tracker.")
        else:
            st.info("Vui lÃ²ng Ä‘Äƒng nháº­p Ä‘á»ƒ sá»­ dá»¥ng tÃ­nh nÄƒng nÃ y.")
