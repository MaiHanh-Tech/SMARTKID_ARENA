import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import time
import json
import hashlib

# === BLOCKS IMPORTS (use services.blocks.* to avoid relative path issues) ===
from services.blocks.file_processor import doc_file
from services.blocks.embedding_engine import load_encoder
from services.blocks.html_generator import load_template, create_html_block, create_interactive_html_block
from services.blocks.rag_orchestrator import (
    analyze_document_streamlit,
    compute_similarity_with_excel,
    store_history,
    init_knowledge_universe,
    create_personal_rag,
    tai_lich_su,
)
from services.blocks.prompts import DEBATE_PERSONAS, BOOK_ANALYSIS_PROMPT
from services.blocks.service_locator import ServiceLocator
from services.blocks.argument_analyzer import ArgumentAnalyzer
from services.blocks.reading_tracker import ReadingProgressTracker

# Optional supabase import (don't fail app if missing)
try:
    from supabase import create_client, Client
except Exception:
    create_client = None

# TRANSLATIONS / UI TEXT (kept as original)
TRANS = {
    "vi": {
        "lang_select": "NgÃ´n ngá»¯ / Language / è¯­è¨€",
        "tab1": "ğŸ“š PhÃ¢n TÃ­ch SÃ¡ch",
        "tab2": "âœï¸ Dá»‹ch Giáº£",
        "tab3": "ğŸ—£ï¸ Tranh Biá»‡n",
        "tab4": "ğŸ™ï¸ PhÃ²ng Thu AI",
        "tab5": "â³ Nháº­t KÃ½",
        "t1_header": "Trá»£ lÃ½ NghiÃªn cá»©u & Knowledge Graph",
        "t1_up_excel": "1. Káº¿t ná»‘i Kho SÃ¡ch (Excel)",
        "t1_up_doc": "2. TÃ i liá»‡u má»›i (PDF/Docx)",
        "t1_btn": "ğŸš€ PHÃ‚N TÃCH NGAY",
        "t1_analyzing": "Äang phÃ¢n tÃ­ch {name}...",
        "t1_connect_ok": "âœ… ÄÃ£ káº¿t ná»‘i {n} cuá»‘n sÃ¡ch.",
        "t1_graph_title": "ğŸª VÅ© trá»¥ SÃ¡ch",
        "t2_header": "Dá»‹ch Thuáº­t Äa Chiá»u",
        "t2_input": "Nháº­p vÄƒn báº£n cáº§n dá»‹ch:",
        "t2_target": "Dá»‹ch sang:",
        "t2_style": "Phong cÃ¡ch:",
        "t2_btn": "âœï¸ Dá»‹ch Ngay",
        "t3_header": "Äáº¥u TrÆ°á»ng TÆ° Duy",
        "t3_persona_label": "Chá»n Äá»‘i Thá»§:",
        "t3_input": "Nháº­p chá»§ Ä‘á» tranh luáº­n...",
        "t3_clear": "ğŸ—‘ï¸ XÃ³a Chat",
        "t4_header": "ğŸ™ï¸ PhÃ²ng Thu AI Äa NgÃ´n Ngá»¯",
        "t4_voice": "Chá»n Giá»ng:",
        "t4_speed": "Tá»‘c Ä‘á»™:",
        "t4_btn": "ğŸ”Š Táº O AUDIO",
        "t5_header": "Nháº­t KÃ½ & Lá»‹ch Sá»­",
        "t5_refresh": "ğŸ”„ Táº£i láº¡i Lá»‹ch sá»­",
        "t5_empty": "ChÆ°a cÃ³ dá»¯ liá»‡u lá»‹ch sá»­.",
    },
    "en": {
        "lang_select": "Language",
        "tab1": "ğŸ“š Book Analysis",
        "tab2": "âœï¸ Translator",
        "tab3": "ğŸ—£ï¸ Debater",
        "tab4": "ğŸ™ï¸ AI Studio",
        "tab5": "â³ History",
        "t1_header": "Research Assistant & Knowledge Graph",
        "t1_up_excel": "1. Connect Book Database (Excel)",
        "t1_btn": "ğŸš€ ANALYZE NOW",
        "t1_analyzing": "Analyzing {name}...",
        "t1_connect_ok": "âœ… Connected {n} books.",
        "t1_graph_title": "ğŸª Book Universe",
        "t2_header": "Multidimensional Translator",
        "t2_input": "Enter text to translate:",
        "t2_target": "Translate to:",
        "t2_style": "Style:",
        "t2_btn": "âœï¸ Translate",
        "t3_header": "Thinking Arena",
        "t3_persona_label": "Choose Opponent:",
        "t3_input": "Enter debate topic...",
        "t3_clear": "ğŸ—‘ï¸ Clear Chat",
        "t4_header": "ğŸ™ï¸ Multilingual AI Studio",
        "t4_voice": "Select Voice:",
        "t4_speed": "Speed:",
        "t4_btn": "ğŸ”Š GENERATE AUDIO",
        "t5_header": "Logs & History",
        "t5_refresh": "ğŸ”„ Refresh History",
        "t5_empty": "No history data found.",
    },
    "zh": {
        "lang_select": "è¯­è¨€",
        "tab1": "ğŸ“š ä¹¦ç±åˆ†æ",
        "tab2": "âœï¸ ç¿»è¯‘ä¸“å®¶",
        "tab3": "ğŸ—£ï¸ è¾©è®ºåœº",
        "tab4": "ğŸ™ï¸ AI å½•éŸ³å®¤",
        "tab5": "â³ å†å²è®°å½•",
        "t1_header": "ç ”ç©¶åŠ©æ‰‹ & çŸ¥è¯†å›¾è°±",
        "t1_up_excel": "1. è¿æ¥ä¹¦åº“ (Excel)",
        "t1_up_doc": "2. ä¸Šä¼ æ–°æ–‡æ¡£ (PDF/Docx)",
        "t1_btn": "ğŸš€ ç«‹å³åˆ†æ",
        "t1_analyzing": "æ­£åœ¨åˆ†æ {name}...",
        "t1_connect_ok": "âœ… å·²è¿æ¥ {n} æœ¬ä¹¦ã€‚",
        "t1_graph_title": "ğŸª ä¹¦ç±å®‡å®™",
        "t2_header": "å¤šç»´ç¿»è¯‘",
        "t2_input": "è¾“å…¥æ–‡æœ¬:",
        "t2_target": "ç¿»è¯‘æˆ:",
        "t2_style": "é£æ ¼:",
        "t2_btn": "âœï¸ ç¿»è¯‘",
        "t3_header": "æ€ç»´ç«æŠ€åœº",
        "t3_persona_label": "é€‰æ‹©å¯¹æ‰‹:",
        "t3_input": "è¾“å…¥è¾©è®ºä¸»é¢˜...",
        "t3_clear": "ğŸ—‘ï¸ æ¸…é™¤èŠå¤©",
        "t4_header": "ğŸ™ï¸ AI å¤šè¯­è¨€å½•éŸ³å®¤",
        "t4_voice": "é€‰æ‹©å£°éŸ³:",
        "t4_speed": "è¯­é€Ÿ:",
        "t4_btn": "ğŸ”Š ç”ŸæˆéŸ³é¢‘",
        "t5_header": "æ—¥å¿— & å†å²",
        "t5_refresh": "ğŸ”„ åˆ·æ–°å†å²",
        "t5_empty": "æš‚æ— å†å²æ•°æ®ã€‚",
    }
}

def T(key):
    lang = st.session_state.get('weaver_lang', 'vi')
    return TRANS.get(lang, TRANS['vi']).get(key, key)

@st.cache_resource
def load_models():
    try:
        model = load_encoder()
        return model
    except Exception:
        return None

def check_model_available():
    model = load_models()
    if model is None:
        st.warning("âš ï¸ Chá»©c nÄƒng Knowledge Graph táº¡m thá»i khÃ´ng kháº£ dá»¥ng (thiáº¿u RAM)")
        return False
    return True

def doc_file_safe(uploaded_file):
    return doc_file(uploaded_file)

# Helper to get or init KnowledgeUniverse without local-name conflicts
def get_knowledge_universe():
    ku = st.session_state.get("knowledge_universe", None)
    if ku is not None:
        return ku
    try:
        ku = init_knowledge_universe()
        st.session_state["knowledge_universe"] = ku
        return ku
    except Exception:
        return None

# Compatibility wrapper for rerun across Streamlit versions â€” minimal fix
def safe_rerun():
    try:
        st.experimental_rerun()
        return
    except Exception:
        pass
    try:
        if hasattr(st, "rerun"):
            st.rerun()
            return
    except Exception:
        pass
    st.session_state["_forced_rerun_flag"] = not st.session_state.get("_forced_rerun_flag", False)
    try:
        st.stop()
    except Exception:
        raise

# ---- Helper: detect consensus but ignore system-error messages ----
def _check_consensus_reached(chat_history):
    if len(chat_history) < 4:
        return False

    last_two = [chat_history[-2]['content'], chat_history[-1]['content']]

    # Ignore if either of last two messages looks like a system / API error
    error_markers = ["Há»‡ thá»‘ng Ä‘ang báº­n", "[System Busy", "âš ï¸ Há»‡ thá»‘ng", "[API Error", "Lá»—i", "System Busy", "exhausted"]
    if any(any(marker in s for marker in error_markers) for s in last_two):
        return False

    encoder = load_models()
    if encoder is not None:
        try:
            embs = encoder.encode(last_two)
            from sklearn.metrics.pairwise import cosine_similarity
            sim = cosine_similarity([embs[0]], [embs[1]])[0][0]
            if sim > 0.85:
                return True
        except Exception:
            # fallback to keyword matching
            pass

    agreement_keywords = ["Ä‘á»“ng Ã½", "Ä‘Ãºng", "thá»«a nháº­n", "agree", "correct", "nháº¥t trÃ­", "thá»‘ng nháº¥t"]
    last_msg = chat_history[-1]['content'].lower()
    if any(kw in last_msg for kw in agreement_keywords):
        return True
    return False

# ---- Helper: persona generation with retry/fallback (module level, not inside loops) ----
def _persona_generate_with_retry(ai_instance, prompt, persona_name, initial_model="pro", max_attempts=3, short_context_limit=800):
    """
    Try to generate persona response with retries and fallbacks.
    Returns tuple (ok:bool, response:str, error_summary:str)
    - ok True when valid response (not system-busy text)
    - response contains AI text when ok, otherwise empty
    - error_summary contains last error string for logging when not ok
    """
    error_markers = ["Há»‡ thá»‘ng Ä‘ang báº­n", "[System Busy", "âš ï¸ Há»‡ thá»‘ng", "[API Error", "Lá»—i", "System Busy", "exhausted"]
    attempt = 0
    last_err = ""

    model_plan = [initial_model, "flash", "flash"]  # fallback sequence
    while attempt < max_attempts:
        model_choice = model_plan[min(attempt, len(model_plan)-1)]
        try:
            res = ai_instance.generate(prompt, model_type=model_choice, system_instruction=DEBATE_PERSONAS.get(persona_name))
        except Exception as e:
            res = f"âš ï¸ Lá»—i khi gá»i AI: {e}"

        # If response looks like system busy / error -> retry
        if not res or any(marker in res for marker in error_markers):
            last_err = res
            try:
                if hasattr(ai_instance, "logger"):
                    ai_instance.logger.log_error("Persona_Generate_Attempt", f"{persona_name} attempt={attempt+1} model={model_choice}", str(res))
            except Exception:
                pass

            time.sleep(0.8 * (attempt + 1))
            # If still failing and we have long prompt, try to shorten context then retry
            if attempt == 1 and len(prompt) > short_context_limit:
                prompt = prompt[-short_context_limit:]  # keep tail (recent context)
            attempt += 1
            continue

        # success; return
        return True, res, ""
    # exhausted attempts
    return False, "", last_err

def detect_contradictions(ku, threshold=0.8):
    if not hasattr(ku, 'episteme_layers') or not hasattr(ku, 'graph'):
        return []

    contradictions = []
    contradiction_pairs = [
        ("Váº­t lÃ½ & Sinh há»c", "Ã thá»©c & Giáº£i phÃ³ng"),
        ("ToÃ¡n há»c & Logic", "VÄƒn hÃ³a & Quyá»n lá»±c")
    ]
    from sklearn.metrics.pairwise import cosine_similarity
    for layer_a, layer_b in contradiction_pairs:
        books_a = ku.episteme_layers.get(layer_a, [])
        books_b = ku.episteme_layers.get(layer_b, [])
        for node_a in books_a:
            for node_b in books_b:
                if node_a not in ku.graph.nodes or node_b not in ku.graph.nodes:
                    continue
                emb_a = ku.graph.nodes[node_a].get("embedding")
                emb_b = ku.graph.nodes[node_b].get("embedding")
                if emb_a is None or emb_b is None:
                    continue
                sim = cosine_similarity([emb_a], [emb_b])[0][0]
                if sim > threshold:
                    contradictions.append({
                        "book_1": ku.graph.nodes[node_a].get("title", node_a),
                        "book_2": ku.graph.nodes[node_b].get("title", node_b),
                        "similarity": float(sim),
                        "tension": f"{layer_a} âš¡ {layer_b}",
                        "explanation": "[Inference] Hai sÃ¡ch nÃ y cÃ¹ng Ä‘á» cáº­p má»™t chá»§ Ä‘á» nhÆ°ng tá»« hai episteme khÃ¡c nhau."
                    })
    return contradictions

def find_related_books_with_decay(ku, query_text, top_k=3):
    if not hasattr(ku, 'graph'):
        return []
    encoder = load_models()
    if not encoder:
        return []
    query_emb = encoder.encode([query_text])[0]
    current_time = datetime.now()

    scored_nodes = []
    for node_id in ku.graph.nodes:
        node = ku.graph.nodes[node_id]
        if "embedding" not in node:
            continue
        base_sim = np.dot(query_emb, node["embedding"]) / (np.linalg.norm(query_emb) * np.linalg.norm(node["embedding"]) + 1e-9)
        added_at_str = node.get("added_at")
        time_factor = 1.0
        if added_at_str:
            try:
                added_time = datetime.fromisoformat(added_at_str)
                days_old = (current_time - added_time).days
                if days_old < 0:
                    days_old = 0
                decay_rate = 0.001
                time_factor = np.exp(-decay_rate * days_old)
            except:
                pass
        score = base_sim * time_factor
        scored_nodes.append((node_id, score))

    scored_nodes.sort(key=lambda x: x[1], reverse=True)
    results = []
    for node_id, score in scored_nodes[:top_k]:
        title = ku.graph.nodes[node_id].get("title", node_id)
        explanation = ku.graph.nodes[node_id].get("summary", "")[:100] + "..."
        results.append((node_id, title, score, explanation))
    return results

# --- RUN ---
def run():
    ai = ServiceLocator.get("ai_core") or ServiceLocator.get("ai_core")
    voice = ServiceLocator.get("voice_engine")

    # initialize knowledge_universe early to avoid UnboundLocalError
    knowledge_universe = get_knowledge_universe()

    with st.sidebar:
        st.markdown("---")
        lang_choice = st.selectbox("ğŸŒ " + TRANS['vi']['lang_select'], ["Tiáº¿ng Viá»‡t", "English", "ä¸­æ–‡"], key="weaver_lang_selector")
        if lang_choice == "Tiáº¿ng Viá»‡t":
            st.session_state.weaver_lang = 'vi'
        elif lang_choice == "English":
            st.session_state.weaver_lang = 'en'
        else:
            st.session_state.weaver_lang = 'zh'

        st.divider()
        if st.button("ğŸ”„ Clear Chat History"):
            st.session_state.weaver_chat = []
            safe_rerun()

    st.header(f"ğŸ§  The Cognitive Weaver")

    tab1, tab2, tab3, tab4, tab5 = st.tabs([T("tab1"), T("tab2"), T("tab3"), T("tab4"), T("tab5")])

    # TAB 1: RAG
    with tab1:
        st.header(T("t1_header"))
        with st.container():
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                file_excel = st.file_uploader(T("t1_up_excel"), type="xlsx", key="t1")
            with c2:
                uploaded_files = st.file_uploader(T("t1_up_doc"), type=["pdf", "docx", "txt", "md", "html"], accept_multiple_files=True)
            with c3:
                st.write("")
                st.write("")
                btn_run = st.button(T("t1_btn"), type="primary", use_container_width=True)

        if btn_run and uploaded_files:
            vec = load_encoder()
            db_df = None
            has_db_excel = False

            if file_excel:
                try:
                    db_df = pd.read_excel(file_excel).dropna(subset=["TÃªn sÃ¡ch"])
                    has_db_excel = True
                    st.success(T("t1_connect_ok").format(n=len(db_df)))
                except Exception as e:
                    st.error(f"âŒ Lá»—i Ä‘á»c Excel: {e}")

            for f in uploaded_files:
                text = doc_file_safe(f)
                if not text:
                    st.warning(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file {f.name}")
                    continue

                link = ""
                if has_db_excel and db_df is not None and vec is not None:
                    try:
                        matches = compute_similarity_with_excel(text, db_df, vec)
                        if matches:
                            link = "\n".join([f"- {m[0]} ({m[1]*100:.0f}%)" for m in matches])
                    except Exception as e:
                        st.warning(f"KhÃ´ng thá»ƒ tÃ­nh similarity: {e}")

                # Re-check knowledge_universe via helper (safe, no UnboundLocalError)
                knowledge_universe = get_knowledge_universe()

                # Safe call: only call methods on knowledge_universe if available
                try:
                    related = knowledge_universe.find_related_books(text[:2000], top_k=3) if knowledge_universe else []
                except Exception as e:
                    st.warning(f"Lá»—i khi tÃ¬m sÃ¡ch liÃªn quan: {e}")
                    related = []

                with st.spinner(T("t1_analyzing").format(name=f.name)):
                    res = analyze_document_streamlit(f.name, text, user_lang=st.session_state.get('weaver_lang', 'vi'))
                    if res and "Lá»—i" not in res:
                        st.markdown(f"### ğŸ“„ {f.name}")
                        if link:
                            st.markdown("**SÃ¡ch cÃ³ liÃªn quan (tá»« Excel):**")
                            st.markdown(link)
                        st.markdown(res)
                        if related:
                            st.markdown("**SÃ¡ch liÃªn quan tá»« Knowledge Graph:**")
                            for node_id, title, score, explanation in related:
                                st.markdown(f"- **{title}** ({score:.2f}) â€” {explanation}")
                        st.markdown("---")
                        store_history("PhÃ¢n TÃ­ch SÃ¡ch", f.name, res[:500])
                    else:
                        st.error(f"âŒ KhÃ´ng thá»ƒ phÃ¢n tÃ­ch file {f.name}: {res}")

        # Graph visualization when Excel provided
        if file_excel:
            try:
                if "df_viz" not in st.session_state:
                    st.session_state.df_viz = pd.read_excel(file_excel).dropna(subset=["TÃªn sÃ¡ch"])
                df_v = st.session_state.df_viz

                with st.expander(T("t1_graph_title"), expanded=False):
                    vec_local = load_models()
                    if vec_local is None:
                        st.warning("âš ï¸ Encoder khÃ´ng táº£i Ä‘Æ°á»£c, bá» qua Ä‘á»“ thá»‹.")
                    else:
                        if "book_embs" not in st.session_state:
                            with st.spinner("Äang sá»‘ hÃ³a sÃ¡ch..."):
                                st.session_state.book_embs = vec_local.encode(df_v["TÃªn sÃ¡ch"].tolist())
                        embs = st.session_state.book_embs
                        sim = np.array([])  # fallback
                        try:
                            from sklearn.metrics.pairwise import cosine_similarity
                            sim = cosine_similarity(embs)
                        except Exception:
                            sim = np.zeros((len(embs), len(embs)))
                        nodes, edges = [], []
                        total_books = len(df_v)
                        c_slider1, c_slider2 = st.columns(2)
                        with c_slider1:
                            max_nodes = st.slider("Sá»‘ lÆ°á»£ng sÃ¡ch hiá»ƒn thá»‹:", 5, total_books, min(50, total_books))
                        with c_slider2:
                            threshold = st.slider("Äá»™ tÆ°Æ¡ng Ä‘á»“ng ná»‘i dÃ¢y:", 0.0, 1.0, 0.45)
                        from streamlit_agraph import agraph, Node, Edge, Config
                        for i in range(min(max_nodes, total_books)):
                            nodes.append(Node(id=str(i), label=df_v.iloc[i]["TÃªn sÃ¡ch"], size=20, color="#FFD166"))
                            for j in range(i+1, min(max_nodes, total_books)):
                                if sim.size and sim[i, j] > threshold:
                                    edges.append(Edge(source=str(i), target=str(j), color="#118AB2"))
                        config = Config(width=900, height=600, directed=False, physics=True, collapsible=False)
                        agraph(nodes, edges, config)

                        knowledge_universe = get_knowledge_universe()
                        if knowledge_universe:
                            contras = detect_contradictions(knowledge_universe, threshold=0.8)
                            if contras:
                                st.error("âš¡ PHÃT HIá»†N MÃ‚U THUáºªN NHáº¬N THá»¨C (Episteme Conflict)")
                                for c in contras:
                                    st.write(f"- **{c['book_1']}** vs **{c['book_2']}** ({c['tension']}): {c['explanation']}")

            except Exception:
                pass

    # TAB 2: Dá»‹ch giáº£
    with tab2:
        st.subheader(T("t2_header"))
        txt = st.text_area(T("t2_input"), height=150, key="w_t2_inp")
        c_l, c_s, c_b = st.columns([1, 1, 1])
        with c_l:
            target_lang = st.selectbox(T("t2_target"), ["Tiáº¿ng Viá»‡t", "English", "Chinese", "French", "Japanese"], key="w_t2_lang")
        with c_s:
            style = st.selectbox(T("t2_style"), ["Default", "Academic", "Literary", "Business"], key="w_t2_style")
        if st.button(T("t2_btn"), key="w_t2_btn") and txt:
            with st.spinner("AI Translating..."):
                p = f"Translate to {target_lang}. Style: {style}. Text: {txt}"
                res = ai.generate(p, model_type="pro")
                st.markdown(res)
                store_history("Dá»‹ch Thuáº­t", f"{target_lang}", txt[:50])

    # TAB 3: Äáº¥u trÆ°á»ng
    with tab3:
        st.subheader(T("t3_header"))
        mode = st.radio("Mode:", ["ğŸ‘¤ Solo", "âš”ï¸ Multi-Agent"], horizontal=True, key="w_t3_mode")
        if "weaver_chat" not in st.session_state:
            st.session_state.weaver_chat = []

        if mode == "ğŸ‘¤ Solo":
            c1, c2 = st.columns([3, 1])
            with c1:
                persona = st.selectbox(T("t3_persona_label"), list(DEBATE_PERSONAS.keys()), key="w_t3_solo_p")
            with c2:
                if st.button(T("t3_clear"), key="w_t3_clr"):
                    st.session_state.weaver_chat = []
                    safe_rerun()
            for msg in st.session_state.weaver_chat:
                st.chat_message(msg.get("role", "user")).write(msg.get("content", ""))
            if prompt := st.chat_input(T("t3_input")):
                st.chat_message("user").write(prompt)
                st.session_state.weaver_chat.append({"role": "user", "content": prompt})
                recent_history = st.session_state.weaver_chat[-10:]
                context_text = "\n".join([f"{m.get('role','').upper()}: {m.get('content','')}" for m in recent_history])
                full_prompt = f"Lá»ŠCH Sá»¬:\n{context_text}\n\nNHIá»†M Vá»¤: Tráº£ lá»i cÃ¢u há»i má»›i nháº¥t cá»§a USER."
                with st.chat_message("assistant"):
                    with st.spinner("..."):
                        res = ai.generate(full_prompt, model_type="flash", system_instruction=DEBATE_PERSONAS[persona])
                        error_markers = ["Há»‡ thá»‘ng Ä‘ang báº­n", "[System Busy", "âš ï¸ Há»‡ thá»‘ng", "[API Error", "Lá»—i", "System Busy"]
                        if not res or any(marker in res for marker in error_markers):
                            st.warning(f"AI tráº£ vá» lá»—i cho persona {persona}: {res}")
                            note = f"(AI lá»—i cho {persona} - Ä‘Ã£ bá» qua. {datetime.now().strftime('%H:%M:%S')})"
                            st.session_state.weaver_chat.append({"role": "assistant", "content": note})
                        else:
                            st.write(res)
                            st.session_state.weaver_chat.append({"role": "assistant", "content": res})
                            store_history("Tranh Biá»‡n Solo", f"{persona} - {prompt[:50]}...", f"Q: {prompt}\nA: {res}")
        else:
            participants = st.multiselect("Chá»n Há»™i Äá»“ng:", list(DEBATE_PERSONAS.keys()),
                                          default=[list(DEBATE_PERSONAS.keys())[0], list(DEBATE_PERSONAS.keys())[1]],
                                          max_selections=3)
            topic = st.text_input("Chá»§ Ä‘á»:", key="w_t3_topic")
            if st.button("ğŸ”¥ KHAI CHIáº¾N", disabled=(len(participants) < 2 or not topic)):
                st.session_state.weaver_chat = []
                start_msg = f"ğŸ“¢ **CHá»¦ Tá»ŒA:** Khai máº¡c tranh luáº­n vá»: *'{topic}'*"
                st.session_state.weaver_chat.append({"role": "system", "content": start_msg})
                st.info(start_msg)
                full_transcript = [start_msg]

                MAX_DEBATE_TIME = 600
                start_time = time.time()

                with st.status("ğŸ”¥ Cuá»™c chiáº¿n Ä‘ang diá»…n ra (3 vÃ²ng)...") as status:
                    try:
                        for round_num in range(1, 4):
                            if time.time() - start_time > MAX_DEBATE_TIME:
                                st.warning("â° Háº¿t giá»! Cuá»™c tranh luáº­n káº¿t thÃºc sá»›m.")
                                break

                            status.update(label=f"ğŸ”„ VÃ²ng {round_num}/3 Ä‘ang diá»…n ra...")

                            for i, p_name in enumerate(participants):
                                if time.time() - start_time > MAX_DEBATE_TIME:
                                    break

                                context_str = topic
                                if len(st.session_state.weaver_chat) > 1:
                                    recent_msgs = st.session_state.weaver_chat[-4:]
                                    context_str = "\n".join([f"{m.get('role','')}: {m.get('content','')}" for m in recent_msgs])

                                length_instruction = " (Báº®T BUá»˜C: Tráº£ lá»i ngáº¯n gá»n khoáº£ng 150-200 tá»«. Äi tháº³ng vÃ o trá»ng tÃ¢m, khÃ´ng lan man.)"

                                if round_num == 1:
                                    p_prompt = f"CHá»¦ Äá»€: {topic}\nNHIá»†M Vá»¤ (VÃ²ng 1 - Má»Ÿ Ä‘áº§u): NÃªu quan Ä‘iá»ƒm chÃ­nh vÃ  2-3 lÃ½ láº½. {length_instruction}"
                                else:
                                    p_prompt = f"CHá»¦ Äá»€: {topic}\nBá»I Cáº¢NH Má»šI NHáº¤T:\n{context_str}\n\nNHIá»†M Vá»¤ (VÃ²ng {round_num} - Pháº£n biá»‡n): Pháº£n biá»‡n sáº¯c bÃ©n quan Ä‘iá»ƒm Ä‘á»‘i thá»§ vÃ  cá»§ng cá»‘ láº­p trÆ°á»ng cá»§a mÃ¬nh. {length_instruction}"

                                try:
                                    ok, res_text, err_text = _persona_generate_with_retry(ai, p_prompt, p_name, initial_model="pro", max_attempts=3)
                                except Exception as e:
                                    ok, res_text, err_text = False, "", f"âš ï¸ Exception: {e}"

                                # If failed after retries -> append short unique note and continue
                                if not ok:
                                    st.warning(f"AI tráº£ vá» lá»—i cho {p_name}: {err_text}")
                                    short_note = f"(AI lá»—i cho {p_name} - vÃ²ng {round_num} - {datetime.now().strftime('%H:%M:%S')})"
                                    st.session_state.weaver_chat.append({"role": "assistant", "content": short_note})
                                    full_transcript.append(short_note)
                                    try:
                                        if hasattr(ai, "logger"):
                                            ai.logger.log_error("Persona_Failure", f"{p_name} round={round_num}", err_text)
                                    except Exception:
                                        pass
                                    continue

                                # Normal flow when response OK
                                clean_res = res_text.replace(f"{p_name}:", "").strip()
                                clean_res = clean_res.replace(f"**{p_name}:**", "").strip()
                                icons = {"Káº» Pháº£n Biá»‡n": "ğŸ˜ˆ", "Shushu": "ğŸ©", "Pháº­t Tá»•": "ğŸ™", "Socrates": "ğŸ›ï¸"}
                                icon = icons.get(p_name, "ğŸ¤–")
                                content_fmt = f"### {icon} {p_name}\n\n{clean_res}"
                                st.session_state.weaver_chat.append({"role": "assistant", "content": content_fmt})
                                full_transcript.append(content_fmt)
                                with st.chat_message("assistant", avatar=icon):
                                    st.markdown(content_fmt)
                                time.sleep(1)

                            # After each round check if consensus reached (but ignore system-error notes)
                            if _check_consensus_reached(st.session_state.weaver_chat):
                                status.update(label="âœ… Tranh luáº­n Ä‘Ã£ Ä‘áº¡t Ä‘á»“ng thuáº­n!", state="complete")
                                st.info("âœ… CÃ¡c bÃªn Ä‘Ã£ tÃ¬m tháº¥y Ä‘iá»ƒm chung (Consensus Reached). Dá»«ng tranh luáº­n.")
                                break

                        status.update(label="âœ… Tranh luáº­n káº¿t thÃºc!", state="complete")
                    except Exception as e:
                        st.error(f"Lá»—i trong quÃ¡ trÃ¬nh tranh luáº­n: {e}")

                full_log = "\n\n".join(full_transcript)
                store_history("Há»™i Äá»“ng Tranh Biá»‡n", f"Chá»§ Ä‘á»: {topic}", full_log)

    # TAB 4: PHÃ’NG THU
    with tab4:
        st.subheader(T("t4_header"))
        inp_v = st.text_area("Text:", height=200)
        btn_v = st.button(T("t4_btn"))
        if btn_v and inp_v and voice:
            path = voice.speak(inp_v)
            if path:
                st.audio(path)

    # TAB 5: NHáº¬T KÃ (CÃ“ PHáº¦N BAYES)
    with tab5:
        st.subheader("â³ Nháº­t KÃ½ & Pháº£n Chiáº¿u TÆ° Duy")
        if st.button("ğŸ”„ Táº£i láº¡i", key="w_t5_refresh"):
            st.session_state.history_cloud = tai_lich_su()
            safe_rerun()

        data = st.session_state.get("history_cloud", tai_lich_su())

        if data:
            df_h = pd.DataFrame(data)

            if "SentimentScore" in df_h.columns:
                try:
                    df_h["score"] = pd.to_numeric(df_h["SentimentScore"], errors='coerce').fillna(0)
                    import plotly.express as px
                    fig = px.line(df_h, x="Time", y="score", markers=True, color_discrete_sequence=["#76FF03"])
                    st.plotly_chart(fig, use_container_width=True)
                except:
                    pass

            with st.expander("ğŸ”® PhÃ¢n tÃ­ch TÆ° duy theo xÃ¡c suáº¥t Bayes (E.T. Jaynes)", expanded=False):
                st.info("AI sáº½ coi Lá»‹ch sá»­ hoáº¡t Ä‘á»™ng cá»§a chá»‹ lÃ  'Dá»¯ liá»‡u quan sÃ¡t' (Evidence) Ä‘á»ƒ suy luáº­n ra 'HÃ m má»¥c tiÃªu' (Objective Function) vÃ  sá»± dá»‹ch chuyá»ƒn niá»m tin cá»§a chá»‹.")

                if st.button("ğŸ§  Cháº¡y MÃ´ hÃ¬nh Bayes ngay"):
                    with st.spinner("Äang tÃ­nh toÃ¡n xÃ¡c suáº¥t háº­u nghiá»‡m (Posterior)..."):
                        recent_logs = df_h.tail(10).to_dict(orient="records")
                        logs_text = pd.io.json.dumps(recent_logs, ensure_ascii=False)

                        bayes_prompt = f"""
                        ÄÃ³ng vai má»™t nhÃ  khoa há»c tÆ° duy theo trÆ°á»ng phÃ¡i E.T. Jaynes (sÃ¡ch 'Probability Theory: The Logic of Science').

                        Dá»® LIá»†U QUAN SÃT (EVIDENCE):
                        ÄÃ¢y lÃ  nháº­t kÃ½ hoáº¡t Ä‘á»™ng cá»§a tÃ´i:
                        {logs_text}

                        NHIá»†M Vá»¤:
                        HÃ£y phÃ¢n tÃ­ch chuá»—i hÃ nh Ä‘á»™ng nÃ y nhÆ° má»™t bÃ i toÃ¡n suy luáº­n Bayes.
                        1. **XÃ¡c Ä‘á»‹nh Priors (Niá»m tin tiÃªn nghiá»‡m):** Dá»±a trÃªn cÃ¡c hÃ nh Ä‘á»™ng Ä‘áº§u, tÃ´i Ä‘ang quan tÃ¢m/tin tÆ°á»Ÿng Ä‘iá»u gÃ¬?
                        2. **Cáº­p nháº­t Likelihood (Kháº£ nÄƒng):** CÃ¡c hÃ nh Ä‘á»™ng tiáº¿p theo cá»§ng cá»‘ hay lÃ m yáº¿u Ä‘i niá»m tin Ä‘Ã³?
                        3. **Káº¿t luáº­n Posterior (Háº­u nghiá»‡m):** Tráº¡ng thÃ¡i tÆ° duy hiá»‡n táº¡i cá»§a tÃ´i Ä‘ang há»™i tá»¥ vá» Ä‘Ã¢u? CÃ³ mÃ¢u thuáº«n (Inconsistency) nÃ o trong logic hÃ nh Ä‘á»™ng khÃ´ng?

                        Tráº£ lá»i ngáº¯n gá»n, sÃ¢u sáº¯c, dÃ¹ng thuáº­t ngá»¯ xÃ¡c suáº¥t nhÆ°ng dá»… hiá»ƒu.
                        """

                        analysis = ai.generate(bayes_prompt, model_type="pro")
                        st.markdown(analysis)

            st.divider()
            for index, item in df_h.iterrows():
                t = str(item.get('Time', ''))
                tp = str(item.get('Type', ''))
                ti = str(item.get('Title', ''))
                ct = str(item.get('Content', ''))

                icon = "ğŸ“"
                if "Tranh Biá»‡n" in tp:
                    icon = "ğŸ—£ï¸"
                elif "Dá»‹ch" in tp:
                    icon = "âœï¸"

                with st.expander(f"{icon} {t} | {tp} | {ti}"):
                    st.markdown(ct)
        else:
            st.info(T("t5_empty"))

    # TAB 6: READING TRACKER
    with tab6:
        st.subheader("ğŸ“Š Tiáº¿n Ä‘á»™ Ä‘á»c sÃ¡ch & Spaced Repetition")
        if "current_user" in st.session_state and st.session_state.current_user:
            try:
                if create_client:
                    url = st.secrets["supabase"]["url"]
                    key = st.secrets["supabase"]["key"]
                    db_client = create_client(url, key)
                    tracker = ReadingProgressTracker(db_client, st.session_state.current_user)

                    due = tracker.get_due_reviews()
                    if due:
                        st.warning(f"â° {len(due)} sÃ¡ch cáº§n Ã´n táº­p!")
                        for rev in due:
                            book_title = "SÃ¡ch"
                            if isinstance(rev.get('reading_progress'), dict):
                                book_title = rev['reading_progress'].get('book_title', 'SÃ¡ch')
                            with st.expander(f"ğŸ“˜ {book_title} (Láº§n {rev['repetition']})"):
                                q = st.slider("Äá»™ nhá»› (0-5):", 0, 5, key=f"q_{rev['book_id']}")
                                if st.button("LÆ°u Ä‘Ã¡nh giÃ¡", key=f"b_{rev['book_id']}"):
                                    tracker.review_book(rev['book_id'], q)
                                    st.success("ÄÃ£ lÆ°u!")
                                    time.sleep(1)
                                    safe_rerun()
                    else:
                        st.success("âœ… Báº¡n Ä‘Ã£ hoÃ n thÃ nh bÃ i Ã´n táº­p hÃ´m nay.")
                else:
                    st.info("Supabase client chÆ°a cáº¥u hÃ¬nh; khÃ´ng thá»ƒ truy váº¥n Reading Tracker.")
            except Exception as e:
                st.error(f"Lá»—i káº¿t ná»‘i DB: {e}")
        else:
            st.info("Vui lÃ²ng Ä‘Äƒng nháº­p Ä‘á»ƒ sá»­ dá»¥ng tÃ­nh nÄƒng nÃ y.")
